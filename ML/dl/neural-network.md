# 神经元

**人工神经元(artificial neuron)**，简称**神经元(neuron)**，是构成神经网络的基本单元，它模拟生物神经元的结构和特性，接收一组输入信号并产生输出。

假设一个神经元接收 $D$ 个输入 $x_1,x_2,\cdots,x_D$，令向量 $\pmb x=[x_1,x_2,\cdots,x_D]$ 来表示这组输入，并用**净输入(net input)** $z\in \mathbb{R}$ 表示一个神经元获得的输入信号 $\pmb x$ 的加权和，
$$
z=\sum_{d=1}^Dw_dx_d+b\\
=\pmb w^{\rm T}\pmb x+b
$$
其中 $\pmb w=[w_1,w_2,\cdots,w_D]\in \mathbb{R}^D$ 是 $D$ 维的权重向量， $b\in\mathbb{R}$ 是偏置。

净输入 $z$ 经过一个非线性函数 $f(\cdot)$ 后，得到神经元的**活性值(activation)** $a$，
$$
a=f(z)
$$
其中非线性函数 $f(\cdot)$ 称为**激活函数(activation function)**。下图给出了典型的神经元结构示例。

![](https://i.loli.net/2020/09/15/YMHfE6OiSJTqrZy.png)

激活函数是神经元中非常重要的部分。为了增强网络的表示能力和学习能力，激活函数需要具备以下性质：

1. 连续并可导(允许少数点上不可导)的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数
2. 激活函数及其导数要尽量简单，以提高网络计算效率
3. 激活函数的导数的值域要在一个合适的区间内，不能过大或过小，否则会影响训练的效率和稳定性。

下面介绍集中常用的激活函数。



## Sigmoid型函数

Sigmoid 型函数是一类S型曲线函数，为两端饱和函数。常用的 Sigmoid 型函数有 Logistic 函数和 Tanh 函数。

> 对于函数 $f(x)$，若 $x\to -\infty$ 时 $f'(x)\to 0$，称其为左饱和；若 $x\to +\infty$ 时 $f'(x)\to 0$，则称其为右饱和；同时满足左饱和和右饱和的函数称为两端饱和。

> Sigmoid 型函数模拟了生物神经元的特点，即对于一些输入产生兴奋，而对于另一些输入产生抑制。



**Logistic 函数**定义为
$$
\sigma(x)=\frac{1}{1+\exp(-x)}
$$


Logistic 函数将一个实数域的输入映射到 $(0,1)$ 区间。当输入值在0附近时，Logistic 函数近似为线性函数。与感知器使用的阶跃激活函数相比，Logistic 函数是连续可导的，数学性质更好。

由于 Logistic 函数的性质，使用 Logistic 激活函数的神经元具有以下性质：

+ 输出可以视作概率分布，使得神经网络可以更好地和统计学习模型结合
+ 可以看作一个**软性门(soft gate)**，用于控制其它神经元输出信息的数量



**Tanh 函数**定义为
$$
\tanh(x)=\frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}
$$
Tanh 函数可以看作 Logistic 函数的放大平移，值域为 $(-1,1)$ 
$$
\tanh(x)=2\sigma(2x)-1
$$
下图给出了 Logistic 函数和 Tanh 函数的形状。Tanh 函数的输出是**零中心化的(zero-centered)**，而 Logistic 函数的输出是非零中心化的。非零中心化的输出会使得后一层的神经元的输入发生**偏置偏移(bias shift)**，从而使得梯度下降的收敛速度变慢。

![](https://i.loli.net/2020/09/15/A9GWxr7Tov1lyqQ.png)



### Hard-Logistic函数和Hard-Tanh函数

Logistic函数和Tanh函数需要计算 $\exp()$ 函数值，使用分段函数近似可以减少计算开销。

Logistic函数在0附近的一阶泰勒展开为
$$
g_l(x)\approx \sigma(0)+\sigma'(0)x=0.25x+0.5
$$
因此可以用分段函数近似
$$
{\rm hard-logistic}(x)=\max\{\min\{0.25x+0.5,1\},0\}
$$
同样的，Tanh函数在0附近的一阶泰勒展开为
$$
g_t(x)\approx \tanh(0)+\tanh'(0)x=x
$$
因此可以用分段函数近似
$$
{\rm hard-logistic}(x)=\max\{\min\{x,1\},-1\}
$$
下图给出了Hard-Logistic函数和Hard-Tanh函数的形状

![](https://i.loli.net/2020/09/15/JHh1LkZD67wxXrV.png)



## ReLU函数

**ReLU(Rectified Linear Unit，修正线性单元)**是目前深度神经网络中经常使用的激活函数。ReLU实际上是一个**斜坡(ramp)**函数，定义为
$$
{\rm ReLU}(x)=\max\{0,x\}
$$
**优点**

+ ReLU只需要进行一次比较，计算上更加高效。

  > ReLU函数也被认为具有生物学上的合理性，比如单侧抑制，宽兴奋边界。
  >
  > 在生物神经网络中，同时处于兴奋状态的神经元非常稀疏，人脑中在同一时刻大概只有 1%∼4% 的神经元处于活跃状态。 Sigmoid 型激活函数会导致一个非稀疏的神经网络，而 ReLU 却具有很好的稀疏性，大约 50% 的神经元会处于激活状态。

+ ReLU函数为左饱和函数，且在 $x>0$ 时导数为1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。

**缺点**

+ ReLU函数的输出是非零中心化的，给后一层的神经网络引入偏置偏移，降低梯度下降的收敛速度。

+ ReLU 神经元在训练时比较容易“死亡”。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个 ReLU 神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是 0，在以后的训练过程中永远不能被激活。这种现象称为死亡 ReLU 问题(Dying ReLU Problem)。



### 带泄露的ReLU

**带泄露的 ReLU (Leaky ReLU)**在输入 $x < 0$ 时，保持一个很小的梯度 $γ$，这样当神经元非激活时也能有一个非零的梯度可以更新参数，避免永远不能被激活 [Maas et al., 2013] 。带泄露的 ReLU 的定义如下：
$$
{\rm LeakyReLU}(x)=\max\{x,\gamma x\}
$$
其中 $\gamma$ 是一个很小的正数，如0.01。



### 带参数的ReLU

**带参数的ReLU(Parametric ReLU , PReLU)**引入一个可学习的参数，不同神经元可以有不同的参数 [He et al., 2015]。对于第 i 个神经元，其 PReLU 的定义为
$$
{\rm PReLU}_i(x)=\max\{0,x\}+\gamma_i \min\{0,x\}
$$
如果 $γ_i = 0$ , 那么 PReLU 就退化为 ReLU；如果 $γ_i$ 是一个很小的正数，则 PReLU 可以看作带泄露的ReLU。



### ELU函数

**ELU(Exponential Linear Unit , 指数线性单元)** [Clevert et al., 2015] 是一个近似的零中心化的非线性函数，其定义为
$$
{\rm ELU}(x)=\max(0,x)+\min(0,\gamma(\exp(x)-1))
$$
其中其中 $\gamma$ 是一个很小的正数，决定 $x\le 0$ 时的饱和曲线。



### Softplus函数

Softplus 函数 [Dugas et al., 2001] 可以看作 Rectifier 函数的平滑版本，其定义为
$$
{\rm Softplus}(x)=\log(1+\exp(x))
$$
Softplus 函数其导数刚好是 Logistic 函数。Softplus 函数虽然也具有单侧抑制、宽兴奋边界的特性，但没有稀疏激活性。



下图给出了ReLU、Leaky ReLU、ELU、Softplus函数的示例

![](https://i.loli.net/2020/09/15/GvKc9BmLChizP7T.png)





## Swish函数

Swish 函数 [Ramachandran et al., 2017] 是一种**自门控(Self-Gated)**激活函数，定义为
$$
{\rm swish}(x) = xσ(βx)
$$
其中 $σ(⋅)$ 为 Logistic 函数， $β$ 为可学习的参数或一个固定超参数。 $σ(⋅) ∈ (0, 1)$ 可以看作一种软性的门控机制：当 $σ(βx)$ 接近于 1 时，门处于“开”状态，激活函数的输出近似于 $x$ 本身；当 $σ(βx)$ 接近于0时，门的状态为“关”，激活函数的输出近似于0。下图给出了 Swish 函数的示例。

![](https://i.loli.net/2020/09/15/sEro4ayYJ5m6fAQ.png)

当 $β = 0$ 时，Swish 函数变成线性函数 $x/2$。当 $β = 1$ 时，Swish 函数在 $x > 0$ 时近似线性，在 $x < 0$ 时近似饱和，同时具有一定的非单调性。当 $β → +∞$ 时， $σ(βx)$ 趋向于单位阶跃函数，Swish 函数近似为 ReLU 函数。因此 Swish 函数可以看作线性函数和 ReLU 函数之间的非线性插值函数，其程度由参数 $β$ 控制。



## GELU函数

**GELU(Gaussian Error Linear Unit , 高斯误差线性单元)** [Hendrycks et al.,2016] 也是一种通过门控机制来调整其输出值的激活函数，和 Swish 函数比较类似
$$
{\rm GELU}(x)=xP(X\le x)
$$
其中 $X$ 服从高斯分布 $\mathcal{N}(\mu,\sigma^2)$，一般设 $\mu=0,\sigma=1$。由于高斯分布的累积分布函数为S型函数，因此 GELU 函数可以用 Tanh 函数或 Logistic 函数来近似
$$
{\rm GELU}(x)\approx 0.5x(1+\tanh(\sqrt{\frac{2}{\pi}}(x+0.044715x^3)))\\
{\rm GELU}(x)\approx x\sigma(1.702x)
$$


## Maxout单元

**Maxout 单元** [Goodfellow et al., 2013] 也是一种分段线性函数。Sigmoid 型函数、ReLU 等激活函数的输入是神经元的净输入 $z$，是一个标量，而 Maxout 单元的输入是上一层神经元的全部原始输出，是一个向量 $\pmb x = [x_1, x_2, ⋯ , x_D ]$。

每个 Maxout 单元有 $K$ 个权重向量 $\pmb w_k\in \mathbb{R}^D$ 和偏置 $b_k(1\le k\le K)$。对于输入 $\pmb x$，可以得到 $K$ 个净输入 $z_k=\pmb w_k^{\rm T}\pmb x+b_k,1\le k\le K$。Maxout 单元的非线性函数定义为
$$
{\rm maxout}(\pmb x)=\max_{k\in \{1,2,\cdots,k\}}(z_k)
$$
Maxout 激活函数可以看作任意凸函数的分段线性近似，并且在有限的点上是不可微的。





# 网络结构

一个生物神经细胞的功能比较简单，而人工神经元只是生物神经细胞的理想化和简单实现，功能更加简单。要想模拟人脑的能力，单一的神经元是远远不够的，需要通过很多神经元一起协作来完成复杂的功能。这样通过一定的连接方式或信息传递方式进行协作的神经元可以看作一个网络，就是神经网络。

> 虽然这里将神经网络结构大体上分为三种类型，但是大多数网络都是复合型结构，即一个神经网络中包括多种网络结构。

## 前馈网络

前馈网络中各个神经元按接收信息的先后分为不同的组，每一组可以看作一个神经层，每一层中的神经元接收前一层神经元的输出，并输出到下一层神经元。整个网络中的信息是朝一个方向传播，没有反向的信息传播，可以用一个有向无环路图表示。

前馈网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射。这种网络结构简单，易于实现。

前馈网络包括全连接前馈网络和卷积神经网络等。



## 记忆网络

记忆网络，也称为反馈网络，网络中的神经元不但可以接收其他神经元的信息，也可以接收自己的历史信息。和前馈网络相比，记忆网络中的神经元具有记忆功能，在不同的时刻具有不同的状态。记忆神经网络中的信息传播可以是单向或双向传递，因此可用一个有向循环图或无向图来表示。

记忆网络可以看作一个程序，具有更强的计算和记忆能力。

记忆网络包括循环神经网络、Hopfield 网络、玻尔兹曼机、受限玻尔兹曼机等。

为了增强记忆网络的记忆容量，可以引入外部记忆单元和读写机制，用来保存一些网络的中间状态，称为记忆增强神经网络(Memory Augmented NeuralNetwork , MANN)，比如神经图灵机 [Graves et al., 2014] 和记忆网络 [Sukhbaatar et al., 2015] 等。



## 图网络

前馈网络和记忆网络的输入都可以表示为向量或向量序列，但实际应用中很多数据是图结构的数据，比如知识图谱、社交网络、分子(molecular)网络等。前馈网络和记忆网络很难处理图结构的数据。

图网络是定义在图结构数据上的神经网络，图中每个节点都由一个或一组神经元构成，节点之间的连接可以是有向的，也可以是无向的，每个节点可以收到来自相邻节点或自身的信息。

图网络是前馈网络和记忆网络的泛化，包含很多不同的实现方式，比如图卷积网络(Graph Convolutional Network, GCN) [Kipf et al., 2016] 、图注意力网络(Graph Attention Network , GAT) [Veličković et al., 2017] 、消息传递神经网络(Message Passing Neural Network ,MPNN) [Gilmer et al., 2017] 等。



下图给出了前馈网络、记忆网络和图网络的网络结构示例，其中圆形节点表示一个神经元，方形节点表示一组神经元。

![](/home/xyx/Pictures/Screenshot from 2020-09-15 13-08-51.png)



