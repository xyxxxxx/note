[toc]

根据通用近似定理，前馈网络和循环网络都有很强的能力。但由于优化算法和计算能力的限制，在实践中很难达到通用近似的能力。特别是在处理复杂任务时，比如需要处理大量的输入信息或者复杂的计算流程时，目前计算机的计算能力依然是限制神经网络发展的瓶颈。

为了减少计算复杂度，通过部分借鉴生物神经网络的一些机制，我们引入了局部连接、权重共享以及汇聚操作来简化神经网络结构（见CNN）。虽然这些机制可以有效缓解模型的复杂度和表达能力之间的矛盾，但是我们依然希望在不“过度”增加模型复杂度（主要是模型参数）的情况下来提高模型的表达能力。以阅读理解任务为例，给定的背景文章一般比较长，如果用循环神经网络来将其转换为向量表示，那么这个编码向量很难反映出背景文章的所有语义。在比较简单的任务（比如文本分类）中，只需要编码一些对分类有用的信息，因此用一个向量来表示文本语义是可行的。但是在阅读理解任务中，编码时还不知道可能会接收到什么样的问句， 这些问句可能会涉及背景文章的所有信息点，因此丢失任何信息都可能导致无法正确回答问题。

> 阅读理解任务是让机器阅读一篇背景文章(background document)，然后询问一些相关的问题， 来测试机器是否理解了这篇文章。

神经网络中可以存储的信息量称为网络容量(network capacity)。一般来讲，利用一组神经元来存储信息时，其存储容量和神经元的数量以及网络的复杂度成正比。要存储的信息越多，神经元数量就要越多或者网络要越复杂，进而导致神经网络的参数成倍地增加。

我们人脑的生物神经网络同样存在网络容量问题，人脑中的工作记忆大概只有几秒钟的时间，类似于循环神经网络中的隐状态。而人脑每个时刻接收的外界输入信息非常多，包括来自于视觉、听觉、触觉的各种各样的信息，单就视觉来说，眼睛每秒钟都会发送千万比特的信息给视觉神经系统。人脑在有限的资源下，并不能同时处理这些<u>过载</u>的输入信息。大脑神经系统有两个重要机制可以解决信息过载问题：<u>注意力</u>和<u>记忆</u>机制。

我们可以借鉴人脑解决信息过载的机制，从两方面来提高神经网络处理信息的能力。一方面是<u>注意力</u>，通过自上而下的信息选择机制来过滤掉大量的无关信息；另一方面是引入额外的<u>外部记忆</u>，优化神经网络的记忆结构来提高神经网络存储信息的容量。



# 认知神经学中的注意力

注意力是一种人类不可或缺的复杂认知功能，指人可以在关注一些信息的同时忽略另一些信息的选择能力。在日常生活中，我们通过视觉、听觉、触觉等方式接收大量的感觉输入。但是人脑还能在这些外界的信息轰炸中有条不紊地工作，是因为人脑可以有意或无意地从这些大量输入信息中选择小部分的有用信息来重点处理，并忽略其他信息。这种能力就叫作**注意力(attention)**。注意力可以作用在外部的刺激（听觉、 视觉、 味觉等），也可以作用在内部的意识（思考、回忆等）。

注意力一般分为两种：

1. 自上而下的有意识的注意力，称为**聚焦式注意力(focus attention)**。聚焦式注意力是指有预定目的、依赖任务的，主动有意识地聚焦于某一对象的注意力。
2. 自下而上的无意识的注意力，称为**基于显著性的注意力(saliency based attention)**。基于显著性的注意力是由外界刺激驱动的注意，不需要主动干预，也和任务无关。如果一个对象的刺激信息不同于其周围信息，一种无意识的 “赢者通吃”(Winner-Take-All)或者门控(gating)机制就可以把注意力转向这个对象。不管这些注意力是有意还是无意，大部分的人脑活动都需要依赖注意力，比如记忆信息、阅读或思考等。

一个和注意力有关的例子是<u>鸡尾酒会效应</u>。当一个人在吵闹的鸡尾酒会上和朋友聊天时，尽管周围噪音干扰很多，他还是可以听到朋友的谈话内容，而忽略其他人的声音（即<u>聚焦式注意力</u>）。同时，如果背景声中有重要的词（比如他的名字），他会马上注意到（即<u>显著性注意力</u>）。

聚焦式注意力一般会随着环境、情景或任务的不同而选择不同的信息。比如当要从人群中寻找某个人时，我们会专注于每个人的脸部；而当要统计人群的人数时，我们只需要专注于每个人的轮廓。





# 注意力机制

在计算能力有限的情况下，**注意力机制(attention mechanism)**作为一种资源分配方案，将有限的计算资源用来处理更重要的信息，是解决信息超载问题的主要手段。

当用神经网络来处理大量的输入信息时，也可以借鉴人脑的注意力机制，只选择一些关键的信息输入进行处理，来提高神经网络的效率。在目前的神经网络模型中，我们可以将最大汇聚(max pooling)、门控(gating)机制近似地看作自下而上的基于显著性的注意力机制。除此之外，自上而下的聚焦式注意力也是一种有效的信息选择方式。以阅读理解任务为例，给定一篇很长的文章，然后就此文章的内容进行提问，提出的问题只和段落中的一两个句子相关，其余部分都是无关的。为了减小神经网络的计算负担，只需要把相关的片段挑选出来让后续的神经网络来处理，而不需要把所有文章内容都输入给神经网络。

用$$X = [\pmb x_1 , ⋯ ,\pmb x_N ] ∈\mathbb{R}^{D×N}$$表示$$N$$组输入信息，其中$$D$$维向量$$\pmb x_n ∈
\mathbb{R}^D, n ∈ [1, N]$$  表示一组输入信息。为了节省计算资源，不需要将所有信息都输入神经网络，只需要从$$X$$中选择一些和任务相关的信息。注意力机制的计算可以分为两步：一是在所有输入信息上计算注意力分布，二是根据注意力分布来计算输入信息的加权平均。

**注意力分布**

为了从$$N$$个输入向量$$[\pmb x_1 , ⋯ ,\pmb x_N ]$$中选择出和某个特定任务相关的信息，我们需要引入一个和任务相关的表示，称为**查询向量(query vector)**，并通过一个打分函数来计算每个输入向量和查询向量之间的相关性。

给定一个和任务相关的查询向量$$\pmb q$$，我们用注意力变量$$z ∈ [1, N]$$来表示被选择信息的索引位置，即$$z = n$$表示选择了第$$n$$个输入向量。为了方便计算，我们采用一种“软性”的信息选择机制。首先计算在给定$$\pmb q$$和$$X$$下，选择第$$i$$个输入向量的概率$$ α_n $$，
$$
\alpha_n=p(z=n|X,\pmb q)\\
={\rm softmax}(s(\pmb x_n,\pmb q))\\
=\frac{\exp(s(\pmb x_n,\pmb q))}{\sum_{j=1}^N \exp(s(\pmb x_j,\pmb q))}
$$
其中$$α_n$$称为**注意力分布(attention distribution)**，$$s(\pmb x,\pmb q)$$为**注意力打分函数**，可以使用以下几种方式来计算：
$$
\begin{align}
加性模型/{\rm additive}/{\rm perceptron}&\quad s(\pmb x,\pmb q)=\pmb v^{\rm T}\tanh(W \pmb x+U\pmb q)\\
点积模型/{\rm dot\ product}&\quad s(\pmb x,\pmb q)=\pmb x^{\rm T}\pmb q\\
缩放点积模型/{\rm scaled\ dot\ product}&\quad s(\pmb x,\pmb q)=\frac{\pmb x^{\rm T}\pmb q}{\sqrt{D}} \\
双线性模型/{\rm general}&\quad s(\pmb x,\pmb q)=\pmb x^{\rm T}W\pmb q
\end{align}
$$
其中$$W,U,\pmb v$$为可学习的参数，$$D$$为输入向量的维度。

理论上，加性模型和点积模型的复杂度差不多，但是点积模型在实现上可以更好地利用矩阵乘积，从而计算效率更高。当输入向量的维度$$D$$比较高时，点积模型的值通常有比较大的方差，从而导致Softmax函数处于梯度非常小的位置。因此，缩放点积模型可以较好地解决这个问题[Vaswani et al., 2017]。

双线性模型是一种泛化的点积模型，假设上式中$$W = U^{\rm T} V$$，双线性模型可以写为$$s(\pmb x,\pmb q) =\pmb x^{\rm T}U^{\rm T}V\pmb q = (U\pmb x)^{\rm T} (V\pmb q)$$, 即分别对$$\pmb x$$和$$\pmb q$$进行线性变换后计算点积。相比点积模型，双线性模型在计算相似度时引入了非对称性。

**加权平均**

注意力分布$$α_n$$可以解释为在给定任务相关的查询$$\pmb q$$时，第$$n$$个输入向量受关注的程度。我们采用一种“软性”的信息选择机制对输入信息进行汇总，即
$$
{\rm att}(X,\pmb q)=\sum_{n=1}^N\alpha_n\pmb x_n=E_{z\sim p(z|X,\pmb q)}(\pmb x_z)
$$
上式称为**软性注意力机制(soft attention mechanism**)。下图给出软性注意力机制的示例。

![](https://i.loli.net/2020/11/09/LsnliCVB4IYPUdv.png)

注意力机制通常用作神经网络中的一个组件。



## 注意力机制的变体

### 硬性注意力

软性注意力选择的信息是所有输入向量在注意力分布下的期望。与之相对的，**硬性注意力(hard attention)**只关注某一个输入向量。硬性注意力有两种实现方式：

1. 选取概率最高的一个输入向量，即
   $$
   {\rm att}(X,\pmb q)=\pmb x_{\hat n}
   $$
   其中$$\hat n = \arg\max_{n=1}^N \alpha_n$$。

2. 在注意力分布式上做随机采样。

硬性注意力的一个缺点是基于最大采样或随机采样的方式来选择信息，使得最终的损失函数与注意力分布之间的函数关系不可导，无法使用反向传播算法进行训练。因此，硬性注意力通常需要使用强化学习来进行训练。为了使用反向传播算法，一般使用软性注意力来代替硬性注意力。



### 键值对注意力

更一般地，我们可以用**键值对(key-value pair)**格式来表示输入信息，其中“键”用来计算注意力分布$$α_n$$，“值”用来计算聚合信息。

用$$(K, V) = [(\pmb k_1 ,\pmb v_1 ), ⋯ , (\pmb k_N ,\pmb v_N)]$$表示$$N$$组输入信息，给定任务相关的查询向量$$q$$时，注意力函数为
$$
{\rm att}((K,V),\pmb q)=\sum_{n=1}^N\alpha_n\pmb v_n\\
=\sum_{n=1}^N {\rm softmax}(s(\pmb k_n,\pmb q))\pmb v_n \\
=\sum_{n=1}^N \frac{\exp(s(\pmb k_n,\pmb q))}{\sum_{j=1}^N \exp(s(\pmb k_j,\pmb q))}\pmb v_n
$$
   其中$$s(\pmb k_n,\pmb q)$$为打分函数。

   

### 多头注意力

**多头注意力(multi-head attention)**是利用多个查询$$Q = [\pmb q_1 , ⋯ ,\pmb q_M ]$$， 来并行地从输入信息中选取多组信息。每个注意力关注输入信息的不同部分。
$$
{\rm att}((K,V),Q)={\rm att}((K,V),\pmb q_1)\oplus\cdots\oplus {\rm att}((K,V),\pmb q_M)
$$
其中$$⊕$$表示向量拼接。



### *指针网络

> 参考论文：Vinyals O, Fortunato M, Jaitly N. Pointer Networks[J]. Computer Science, 2015, 28.
>
> 文章：[Pointer Networks简介及其应用](https://zhuanlan.zhihu.com/p/48959800)

注意力机制可以分为两步：一是计算注意力分布$$α$$，二是根据$$α$$来计算输入信息的加权平均。我们可以只利用注意力机制中的第一步，将注意力分布作为一个软性的**指针(pointer)**来指出相关信息的位置。

**指针网络(pointer network)** [Vinyals et al., 2015] 是一种序列到序列模型，输入是长度为$$N$$的向量序列$$X =\pmb x_1 , ⋯ ,\pmb x_N$$，输出是长度为$$M$$的下标序列$$\pmb c_{1∶M} = c_1 , c_2 , ⋯ , c_M , c_m ∈ [1, N], ∀m$$。

和一般的序列到序列任务不同, 这里的输出序列是输入序列的下标（索引）。比如输入一组乱序的数字，输出为按大小排序的输入数字序列的下标，例如输入为 20, 5, 10，输出为 1, 3, 2。

指针网络的具体实现方法为：

……





# 自注意力模型

当使用神经网络来处理一个变长的向量序列时，我们通常可以使用卷积网络或循环网络进行编码来得到一个相同长度的输出向量序列，如下图所示。

![](https://i.loli.net/2020/11/09/wI8tRnVz6GoKuT7.png)

基于卷积或循环网络的序列编码都是一种局部的编码方式，只建模了输入信息的局部依赖关系。虽然循环网络理论上可以建立长距离依赖关系，但是由于信息传递的容量以及梯度消失问题，实际上也只能建立短距离依赖关系。

如果要建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互；另一种方法是使用全连接网络，全连接网络是一种非常直接的建模远距离依赖的模型，但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的，这时我们就可以利用注意力机制来 “动态” 地生成不同连接的权重，这就是**自注意力模型(self-attention model)**。

为了提高模型能力，自注意力模型经常采用**查询-键-值(Query-Key-Value, QKV)**模式，其计算过程如下图所示，其中红色字母表示矩阵的维度。

![](https://i.loli.net/2020/11/09/godBrpMX85tTAu9.png)

假设输入序列为$$X = [\pmb x_1 , ⋯ ,\pmb x_N] ∈\mathbb{R}^{D_x×N}$$，输出序列为$$H = [\pmb h_1 , ⋯ ,\pmb h_N ] ∈\mathbb{R}^{D_v ×N}$$，自注意力模型的具体计算过程如下：

1. 对于每个输入$$\pmb x_i$$，我们首先将其线性映射到三个不同的空间，得到查询向量$$\pmb q_i\in\mathbb{R}^{D_k}$$，键向量$$\pmb k_i\in\mathbb{R}^{D_k}$$和值向量$$\pmb v_i\in\mathbb{R}^{D_v}$$，映射过程即为
   $$
   Q=W_qX\in \mathbb{R}^{D_k\times N}\\
   K=W_kX\in \mathbb{R}^{D_k\times N}\\
   V=W_vX\in \mathbb{R}^{D_v\times N}\\
   $$
   其中$$W_q,W_k,W_v$$为参数矩阵，$$Q=[\pmb q_1 , ⋯ ,\pmb q_N ],K=[\pmb k_1 , ⋯ ,\pmb k_N ],V=[\pmb v_1 , ⋯ ,\pmb v_N ]$$分别是由<u>查询向量</u>、<u>键向量</u>和<u>值向量</u>构成的矩阵。

2. 对于每一个查询向量$$\pmb q_n\in Q$$，利用键值对注意力机制公式，可以得到输出向量$$\pmb h_n$$，
   $$
   \pmb h_n={\rm att}((K,V),\pmb q_n)\\
   =\sum_{i=1}^N \alpha_{ni}\pmb v_i \\
   =\sum_{i=1}^N {\rm softmax}(s(\pmb k_i,\pmb q_n))\pmb v_i\\
   =\sum_{i=1}^N \frac{\exp(s(\pmb k_i,\pmb q_n))}{\sum_{j=1}^N \exp(s(\pmb k_j,\pmb q_n))}\pmb v_i
   $$
   其中$$n,i\in[1,N]$$为输出和输入向量序列的位置，

如果使用缩放点积作为注意力打分函数，输出向量序列可以简写为
$$
H=V {\rm softmax}(\frac{K^{\rm T}Q}{\sqrt{D_k}})
$$
其中$${\rm softmax}()$$函数按列进行归一化。

下图给出全连接模型和自注意力模型的对比，其中实线表示可学习的权重，虚线表示动态生成的权重。由于自注意力模型的权重是动态生成的，因此可以处理变长的信息序列。

![](https://i.loli.net/2020/11/09/ep5A1CQMu9GHvIV.png)

自注意力模型可以作为神经网络中的一层来使用，既可以用来替换卷积层和循环层 [Vaswani et al., 2017]，也可以和它们一起交替使用(比如$$X$$可以是卷积层或循环层的输出)。自注意力模型计算的权重$$α_{ij}$$只依赖于$$q_i$$和$$k_j$$的相关性，而忽略了输入信息的位置信息，因此在单独使用时，自注意力模型一般需要加入位置编码信息来进行修正 [Vaswani et al., 2017]。自注意力模型可以扩展为多头自注意力(multi-head self-attention)模型，在多个不同的投影空间中捕捉不同的交互信息。





# *人脑中的记忆

在生物神经网络中，记忆是外界信息在人脑中的存储机制。大脑记忆毫无疑问是通过生物神经网络实现的，虽然其机理目前还无法解释，但直观上记忆机制和神经网络的连接形态以及神经元的活动相关。生理学家发现信息是作为一种整体效应(collective effect)存储在大脑组织中，当大脑皮层的不同部位损伤时，其导致的不同行为表现似乎取决于损伤的程度而不是损伤的确切位置[Kohonen, 2012]，大脑组织的每个部分似乎都携带一些导致相似行为的信息。也就是说，记忆在大脑皮层是分布式存储的，而不是存储于某个局部区域 [Thompson, 1975]。

人脑中的记忆具有<u>周期性</u>和<u>联想性</u>。

**记忆周期**

虽然我们还不清楚人脑记忆的存储机制，但是已经大概可以确定不同脑区参与了记忆形成的几个阶段。人脑记忆的一个特点是，记忆一般分为长期记忆和短期记忆。**长期记忆(long-term memory)**，也称为**结构记忆**或**知识**
**(knowledge)**，体现为神经元之间的连接形态，其更新速度比较慢。**短期记忆(short-term memory)**体现为神经元的活动，更新较快，维持时间为几秒至几分钟。短期记忆是神经连接的暂时性强化，通过不断巩固、强化可形成长期记忆。短期记忆、长期记忆的动态更新过程称为**演化(evolution)**过程。

因此，长期记忆可以类比于人工神经网络中的权重参数，而短期记忆可以类比于人工神经网络中的隐状态。

除了长期记忆和短期记忆，人脑中还会存在一个“缓存”，称为**工作记忆(working memory)**。在执行某个认知行为（比如记下电话号码，做算术运算）时，工作记忆是一个记忆的临时存储和处理系统，维持时间通常为几秒钟。从时间上看，工作记忆也是一种短期记忆，但和短期记忆的内涵不同。短期记忆一般指外界的输入信息在人脑中的表示和短期存储，不关心这些记忆如何被使用；而工作记忆是一个和任务相关的 “容器”，可以临时存放和某项任务相关的短期记忆和其他相关的内在记忆。工作记忆的容量比较小，一般可以容纳 4 组项目。

作为不严格的类比，现代计算机的存储也可以按照不同的周期分为不同的存储单元，比如寄存器、内存、外存（比如硬盘等）。

**联想记忆**

大脑记忆的一个主要特点是通过联想来进行检索的。**联想记忆(associative memory)**是指一种学习和记住不同对象之间关系的能力，比如看见一个人然后想起他的名字，或记住某种食物的味道等。

联想记忆是指一种可以通过内容匹配的方法进行寻址的信息存储方式，也称为基于内容寻址的存储(Content Addressable Memory, CAM)。作为对比，现代计算机的存储方式是根据地址来进行存储的，称为随机访问存储(Random Access Memory, RAM)。



和之前介绍的 LSTM 中的记忆单元相比，外部记忆可以存储更多的信息，并且不直接参与计算，通过读写接口来进行操作。而 LSTM 模型中的记忆单元包含了信息存储和计算两种功能，不能存储太多的信息。因此，LSTM 中的记忆单元可以类比于计算机中的寄存器，而外部记忆可以类比于计算机中的内存单元。

借鉴人脑中工作记忆，可以在神经网络中引入一个外部记忆单元来提高网络容量。外部记忆的实现途径有两种：一种是结构化的记忆，这种记忆和计算机中的信息存储方法比较类似，可以分为多个记忆片段，并按照一定的结构来存储；另一种是基于神经动力学的联想记忆，这种记忆方式具有更好的生物学解释性。

下表给出了不同领域中记忆模型的不严格类比：

| 记忆周期 | 计算机   | 人脑     | 神经网络         |
| -------- | -------- | -------- | ---------------- |
| 短期     | 寄存器   | 短期记忆 | 状态(神经元活性) |
| 中期     | 内存     | 工作记忆 | 外部记忆         |
| 长期     | 外存     | 长期记忆 | 可学习参数       |
| 存储方式 | 随机寻址 | 内容寻址 | 内容寻址为主     |



# 序列到序列模型

在序列生成任务中，有一类任务是序列到序列生成任务，即输入一个序列，生成另一个序列，比如机器翻译、语音识别、文本摘要、对话系统、图像标题生成等。

**序列到序列(Sequence-to-Sequence , Seq2Seq)**是一种条件的序列生成问题，给定一个序列$$\pmb x_{1∶S}$$，生成另一个序列$$\pmb y_{1∶T}$$。输入序列的长度$$S$$和输出序列的长度$$T$$可以不同，比如在机器翻译中，输入为源语言，输出为目标语言。下图给出了基于循环神经网络的序列到序列机器翻译示例，其中$$⟨EOS⟩$$表示输入序列的结束，虚线表示用上一步的输出作为下一步的输入。

![](https://i.loli.net/2020/11/10/jHZJGxo6Wmc7aSn.png)

序列到序列模型的目标是估计条件概率
$$
p_\theta(\pmb y_{1∶T}|\pmb x_{1∶S})=\prod_{t=1}^T p_{\theta}(\pmb y_t|\pmb y_{1∶(t-1)},\pmb x_{1∶S})
$$
其中$$\pmb y_t ∈\mathcal{V}$$为词表$$\mathcal{V}$$中的某个词。

给定一组训练数据$$\{(\pmb x_{S_n}, \pmb y_{T_n})\}^N_{n=1}$$，我们可以使用最大似然估计来训练模型参数
$$
\hat{\theta}=\arg\max_\theta\sum_{n=1}^N\log p_\theta(\pmb y_{1:T_n}|\pmb x_{1:S_n})
$$
一旦训练完成，模型就可以根据一个输入序列$$x$$来生成最可能的目标序列
$$
\hat{\pmb y} =\arg\max_y p_{\hat{\theta}}(\pmb y|\pmb x)
$$
具体的生成过程可以通过贪婪方法或束搜索来完成。

和一般的序列生成模型类似，条件概率$$p_θ (y_t|\pmb y_{1∶(t−1)},\pmb x_{1∶S})$$可以使用各种不同的神经网络来实现。这里我们介绍三种主要的序列到序列模型：基于循环神经网络的序列到序列模型、基于注意力的序列到序列模型、基于自注意力的序列到序列模型。



## 基于循环神经网络的序列到序列模型

实现序列到序列的最直接方法是使用两个循环神经网络来分别进行编码和解码，也称为**编码器 - 解码器(encoder-decoder)**模型。

**编码器**

首先使用一个循环神经网络$$f_{\rm enc}$$来编码输入序列$$\pmb x_{1∶S}$$得到一个固定维数的向量$$\pmb u$$，$$\pmb u$$一般为<u>编码循环神经网络最后时刻的隐状态</u>。
$$
\overline{\pmb h}_s=f_{\rm enc}(\overline{\pmb h}_s-1,\pmb x_{s-1},\theta_{\rm enc}),\ s=1,\cdots,S\\
\pmb c=\overline{\pmb h}_S
$$
其中$$f_{\rm enc}(⋅)$$为<u>编码循环神经网络</u>，可以是 LSTM 或 GRU ，其参数为$$\theta_{\rm enc}$$，$$\pmb x_{s-1}$$为词$$x$$的词向量；$$\pmb c$$称为上下文向量(context vector)。

**解码器**

在生成目标序列时，使用另外一个循环神经网络$$f_{\rm dec}$$来进行解码。在解码过程的第$$t$$步时，已生成前缀序列为$$\pmb y_{1:T_n}$$ . 令$$\overline{\pmb h}_t$$表示在网络$$f_{\rm dec}$$的隐状态，$$\pmb o_t ∈ (0, 1)^{|\mathcal{V}|}$$为词表中所有词的后验概率，则
$$
\pmb h_0=\pmb c=\overline{\pmb h}_S\\
\pmb h_t=f_{\rm dec}(\pmb h_{t-1},\pmb y_{t-1},\theta_{\rm dec})\\
\pmb o_t=g(\overline{\pmb h}_t,\theta_o),\ t=1,\cdots,T
$$
其中$$f_{\rm dec}(\cdot)$$为<u>解码循环神经网络</u>，$$g(⋅)$$为最后一层为 Softmax 函数的前馈神经网络，$$\theta_{\rm dec}$$和$$θ_o$$为网络参数，$$\pmb y_{t-1}$$为词$$y$$的词向量，$$\pmb y_0$$为一个特殊符号，比如$$⟨EOS⟩$$。

基于循环神经网络的序列到序列模型的缺点是：(1) 编码向量$$\pmb u$$的容量问题，输入序列的信息很难全部保存在一个固定维度的向量中； (2) 当序列很长时， 由于循环神经网络的长程依赖问题，容易丢失输入序列的信息。



## 基于注意力的序列到序列模型

为了获取更丰富的输入序列信息，我们可以在每一步中通过注意力机制来从输入序列中选取有用的信息。

在解码过程的第$$t$$步中，先用上一步的隐状态$$\pmb h_{t-1}$$作为查询向量，利用注意力机制从所有输入序列的隐状态$$H_{\rm enc}=[\overline{\pmb h}_1,\cdots,\overline{\pmb h}_S]$$中选择相关信息
$$
\pmb c_t={\rm att}(H_{\rm enc},\pmb h_{t-1})\\
=\sum_{i=1}^S \alpha_{i}\overline{\pmb h}_i \\
=\sum_{i=1}^S {\rm softmax}(s(\overline{\pmb h}_i,\pmb h_{t-1}))\overline{\pmb h}_i\\
$$
其中$$\pmb c_t$$称为上下文向量，$$s(\cdot)$$为注意力打分函数。

然后将从输入序列中选择的信息$$\pmb c_t$$也作为解码器$$f_{\rm dec}(\cdot)$$在第$$t$$步时的输入，得到第$$t$$步的隐状态
$$
\pmb h_t=f_{\rm dec}(\pmb h_{t-1},[\pmb y_{t-1}, \pmb c_t],\theta_{\rm dec})\\
$$
最后将$$\overline{\pmb h}_t$$输入到分类器$$g(\cdot)$$中来预测词表中每个词出现的概率。



### Bahdanau Attention

> 参考论文[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)

Bahdanau attention是由Bahdanau等人在2015年提出来的一种注意力机制（也是attention首次应用于NLP），其结构仍然采用encoder-decoder形式，如下图所示。

![](https://images2015.cnblogs.com/blog/670089/201610/670089-20161012111503843-584496480.png)

论文中编码器采用的是双向循环神经网络结构；解码器的attention机制设计如下：

![](https://i.loli.net/2020/11/11/CWIYdh8l5ZH3Awi.png)

首先定义条件概率
$$
p(\pmb y_i|\pmb y_{1∶(i-1)},\pmb x_{1∶S})=g(\pmb y_{i-1},\pmb s_i,\pmb c_i)
$$
其中$$\pmb s_i$$是解码循环神经网络的隐状态，由下式计算
$$
\pmb s_i=f(\pmb s_{i-1},\pmb y_{i-1},\pmb c_i)
$$
上下文向量$$c_i$$由编码循环神经网络的隐状态加权求和得到
$$
\pmb c_i=\sum_{j=1}^T\alpha_{ij}\pmb h_j\\
=\sum_{j=1}^T{\rm softmax}({\rm score}(\pmb s_{i-1},\pmb h_j))\pmb h_j
$$
其中$$h_j$$由双向循环神经网络分别计算的隐状态拼接得到，即$$\pmb h_j=\vec{\pmb h_j}\oplus\overleftarrow{\pmb h_j}$$。

实验结果显示，相比传统的encoder-decoder模型，使用注意力机制的模型的表现更好，并且受输入句子长度增加的影响更小甚至几乎没有影响，表明在长句的处理上更有优势。

![](https://i.loli.net/2020/12/17/wMyD5C3ZobQU6cN.png)

此外，它还可以可视化注意力分布，使模型的翻译过程更具有解释力。

![](https://i.loli.net/2020/12/17/KZqekzTJwfhI2B3.png)



### Luong Attention

> 参考论文[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025v5.pdf)

Luong attention也是在2015年由Luong提出来的一种注意力机制。Luong在论文中提出了两种类型的注意力机制：一种是全局注意力模型，即每次计算上下文向量时都考虑输入序列的所有隐状态；另一种是局部注意力模型，即每次计算上下文向量时只考虑输入序列隐状态中的一个子集。

Luong attention的模型结构也是采用encoder-decoder的形式，encoder和decoder均采用多层LSTM，如下图所示。对于全局注意力模型和局部注意力模型，在计算上下文向量时，均使用encoder和decoder最顶层的LSTM的隐状态。

![](https://i.loli.net/2020/11/11/WBnJPoQX8Tmdy9V.png)



**全局注意力模型**

全局注意力模型在计算decoder的每个时间步的上下文向量$$\pmb c_t$$时，均考虑encoder的所有隐状态，记每个时间步对应的权重向量为$$\pmb a_t$$，其计算公式如下：
$$
\pmb a_t(s)={\rm align}(\pmb h_t,\overline{\pmb h}_s)\\
={\rm softmax}({\rm score}(\pmb h_t,\overline{\pmb h}_s))
$$
其中，$$\pmb h_t$$表示当前decoder第$$t$$个时间步的隐状态，$$\overline{\pmb h}_s$$表示encoder第$$s$$个时间步的隐状态。Luong attention在计算权重时提供了三种计算方式，并且发现<u>对于全局注意力模型，采用dot的权重计算方式效果要更好</u>：
$$
{\rm score}(\pmb h_t,\overline{\pmb h}_s)=

\left\{ 
    \begin{array}{l}
        \pmb h_t^{\rm T} \overline{\pmb h}_s,\quad\quad {\rm dot} \\ 
        \pmb h_t^{\rm T}W_a \overline{\pmb h}_s,\ \ {\rm general} \\ 
        \pmb v_a^{\rm T}\tanh(W_a[\pmb h_t;\overline{\pmb h}_s]),\ \ {\rm concat}\\
    \end{array}
\right.
$$
其中，concat模式跟Bahdanau attention的计算方式一致，而dot和general则直接采用矩阵乘积的形式。在计算完权重向量$$\pmb a_t$$后，将其对encoder的隐状态进行加权平均得到此刻的上下文向量$$\pmb c_t$$，
$$
\pmb c_t=\pmb a_t(s)^{\rm T}\overline{\pmb h}_s
$$
然后Luong attention将其与decoder此刻的隐状态$$\pmb h_t$$进行拼接，并通过一个带有tanh的全连接层得到$$\tilde{\pmb h}_t$$：
$$
\tilde{\pmb h}_t=\tanh(W_c[\pmb c_t;\pmb h_t])
$$
最后，将$$\tilde{\pmb h}_t$$传入带有softmax的输出层即可得到此刻目标词汇的概率分布：
$$
p(\pmb y_t|\pmb y_{1∶(t-1)},\pmb x)={\rm softmax}(W_s\tilde{\pmb h}_t)
$$



**局部注意力模型**

然而，全局注意力模型由于在每次decoder时，均考虑encoder所有的隐状态，因此其计算成本是非常昂贵的，特别是对于一些长句子或长篇文档，其计算就变得不切实际。因此作者又提出了另一种注意力模式，即局部注意力模型，即每次decoder时不再考虑encoder的全部隐状态了，只考虑局部的隐状态。

在局部注意力模型中，在decoder的每个时间步$$t$$，需要先确定输入序列中与该时刻对齐的一个位置$$p_t$$，然后以该位置为中心设定一个窗口大小，即$$[p_t-D,p_t+D]$$，其中$$D$$是表示窗口大小的整数，具体的取值需要凭经验设定，作者在论文中设定的是10。接着在计算权重向量时，只考虑encoder中在该窗口内的隐状态，当窗口的范围超过输入序列的范围时，则对超出的部分直接舍弃。局部注意力模型的计算逻辑如下图所示。

![](https://i.loli.net/2020/11/11/CUfWaAhH13qvc6m.png)

 在确定位置$$p_t$$时，作者也提出了两种对齐方式，一种是单调对齐，一种是预测对齐，分别定义如下：

+ 单调对齐(local-m)：即直接设定$$p_t=t$$，该对齐方式假设输入序列与输出序列的按时间顺序对齐的，接着计算$$\pmb a_t$$的方式与全局注意力模型相同。

+ 预测对齐(local-p)：预测对齐在每个时间步时会对$$p_t$$进行预测，其计算公式如下：
  $$
  p_t=S\cdot {\rm sigmoid}(\pmb v_p^{\rm T}\tanh(W_p \pmb h_t))
  $$
  其中，$$W_p,\pmb v_p$$为参数，$$S$$为输入序列的长度，这样一来，$$p_t\in [0,S]$$。另外在计算$$\pmb a_t$$时，作者还采用了高斯分布进行修正，其计算公式如下：
  $$
  \pmb a_t(s)={\rm align}(\pmb h_t,\overline{\pmb h}_s)\exp(-\frac{(s-p_t)^2}{2\sigma^2})
  $$
  其中，$${\rm align}(\pmb h_t,\overline{\pmb h}_s)$$与全局注意力模型的计算公式相同，$$s$$表示输入序列的位置，$$\sigma=D/2$$。

计算完权重向量后，后面$$\pmb c_t,\ \tilde{\pmb h}_t$$以及概率分布的计算都与全局注意力模型的计算方式相同，这里不再赘述。作者在实验中发现局部注意力模型采用local-p的对齐方式往往效果更好，因为在真实场景中输入序列和输出序列往往不会严格单调对齐，比如在翻译任务中，往往两种语言在表述同样一种意思时，其语序是不一致的。另外，计算权重向量的方式采用general的方式效果比较好。



**input-feeding**

在包含注意力机制的模型中，前面的词汇的对齐过程往往对后续词汇的对齐和预测是有帮助的。为此作者提出了input-feeding方法，即把上一个时刻的$$\tilde{\pmb h}_t$$与下一个时刻的输入进行拼接，共同作为下一个时刻的输入，如下图所示。实验结果显示，此方法可以显著提高decoder的效果。

![](https://i.loli.net/2020/11/11/JZr9MzaD5lcVmhS.png)



### 比较

1. Luong attention使用encoder和decoder的stacked LSTM最上层的隐状态，而Bahdanau attention使用encoder的BiRNN的隐状态拼接和decoder的non-stacking RNN的隐状态
2. Luong attention的计算路径是$$\pmb h_t\to \pmb a_t\to \pmb c_t\to \tilde{\pmb h}_t$$，而Bahdanau attention的计算路径是$$\pmb h_{t-1}\to \pmb a_t\to \pmb c_t\to \pmb h_t$$，并且之后进行预测的处理也有所不同

<img src="https://i.stack.imgur.com/yqJpG.png" style="zoom:150%;" />



## 基于自注意力的序列到序列模型

> 参考论文[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

除长程依赖问题外，基于循环神经网络的序列到序列模型的另一个缺点是无法并行计算。为了提高并行计算效率以及捕捉长距离的依赖关系，我们可以使用**自注意力模型(self-attention model)**来建立一个全连接的网络结构。本节介绍一个目前非常成功的基于自注意力的序列到序列模型：Transformer [Vaswani et al., 2017]。



### 自注意力

对于一个向量序列$$H=[\pmb h_1,\cdots,\pmb h_{T}]\in \mathbb{R}^{D_h\times T}$$，首先用自注意力模型对其进行编码，即
$$
{\rm selfatt}(Q,K,V)=V{\rm softmax}(\frac{K^{\rm T}Q}{\sqrt{D_k}})\in\mathbb{R}^{D_v\times T}\\
Q=W_qH,K=W_kH,V=W_vH
$$
其中$$D_k$$是矩阵$$Q,K$$的列向量的维度，$$W_q\in\mathbb{R}^{D_k\times D_h},W_k\in\mathbb{R}^{D_k\times D_h},W_v\in\mathbb{R}^{D_v\times D_h}$$是三个投影矩阵。



### 多头自注意力

自注意力模型可以看作在一个线性投影空间中建立$$H$$中不同向量之间的关系。为了提取更多的关系信息，我们可以使用多头自注意力(multi-head self-attention)，在多个不同的投影空间中捕捉不同的关系信息。假设在$$M$$个投影空间中分别应用自注意力模型，有
$$
{\rm MultiHead}(H)=W_o[{\rm head}_1;{\rm head}_M]\in \mathbb{R}^{D_h\times T}\\
{\rm head}_m={\rm selfatt}(Q_m,K_m,V_m)\in\mathbb{R}^{D_v\times T}\\
Q_m=W_{q,m}H,K_m=W_{k,m}H,V_m=W_{v,m}H,\ m=1,2,\cdots,M
$$
其中$$W_o\in\mathbb{R}^{D_h\times MD_v}$$是输出投影矩阵，$$W_{q,m}\in\mathbb{R}^{D_k\times D_h},W_{k,m}\in\mathbb{R}^{D_k\times D_h},W_{v,m}\in\mathbb{R}^{D_v\times D_h}$$是投影矩阵。



### 基于自注意力模型的序列编码

对于序列$$\pmb x_{1∶T}$$，我们可以构建一个含有多层多头自注意力模块的模型来对其进行编码。由于自注意力模型没有循环(recurrence)或者卷积，为了利用序列的顺序信息，我们需要在序列中注入位置信息，方法是在第1层的输入序列加上**位置编码(positional encoding)**。

对于一个输入序列$$\pmb x_{1∶T}\in\mathbb{R}^{D\times T}$$，令
$$
H^{(0)}=[\pmb x_1+\pmb p_1,\cdots,\pmb x_T+\pmb p_T]
$$
其中$$\pmb p_t\in\mathbb{R}^D$$为位置$$t$$的向量表示，即位置编码。$$\pmb p_t$$可以作为学习的参数，也可以预定义值，这里使用：
$$
\pmb p_{t,2i}=\sin(t/10000^{2i/D})\\
\pmb p_{t,2i+1}=\cos(t/10000^{2i/D})
$$
其中$$\pmb p_{t,2i}$$表示第$$t$$个位置的编码向量的第$$2i$$维，D 是编码向量的维度。

给定第$$l − 1$$层的隐状态$$H^{(l−1)}$$，第$$l$$层的隐状态$$H^{(l)}$$可以通过一个多头自注意力模块和一个非线性的前馈网络得到。每次计算都需要残差连接以及层归一化操作，具体计算为
$$
Z^{(l)}={\rm norm}(H^{(l-1)}+{\rm MultiHead}(H^{(l-1)}))\\
H^{(l)}={\rm norm}(Z^{(l)}+{\rm FFN}(Z^{(l)}))
$$
其中$${\rm norm}(\cdot)$$表示层归一化，$${\rm FFN}(\cdot)$$表示**逐位置的前馈神经网络(position-wise feed-forward network)**，是一个简单的两层网络。对于输入序列中每个位置上向量$$\pmb z\in Z^{(l)}$$，
$$
{\rm FFN}(\pmb z)=W_2+{\rm ReLu}(W_1\pmb z+\pmb b_1)+\pmb b_2
$$
其中$$W_1,W_2,\pmb b_1,\pmb b_2$$为网络参数。

基于自注意力模型的序列编码可以看作一个全连接的前馈神经网络，第$$l$$层的每个位置都接受第$$l − 1$$层的所有位置的输出。不同的是，其连接权重是通过注意力机制动态计算得到。



### *Transformer模型

Transformer 模型 [Vaswani et al., 2017] 是一个基于多头自注意力的序列到序列模型……





# *记忆增强神经网络

为了增强网络容量，我们可以引入辅助记忆单元，将一些和任务相关的信息保存在辅助记忆中，在需要时再进行读取，这样可以有效地增加网络容量。这个引入的辅助记忆单元一般称为**外部记忆(external memory)**，以区别于循环神经网络的内部记忆（即隐状态）。这种装备外部记忆的神经网络也称为**记忆增强神经网络(Memory Augmented Neural Network, MANN)**，或简称为**记忆网络(Memory Network, MN)**。

记忆网络的典型结构如下图所示，一般由以下几个模块构成：

![](https://i.loli.net/2020/11/10/gFveWwVIs15ypdt.png)

1. 主网络$$C$$：也称为**控制器(controller)**，负责信息处理以及与外界的交互（接受外界的输入信息并产生输出到外界）。主网络通过读写模块和外部记忆进行交互。
2. 外部记忆单元$$M$$：外部记忆单元用来存储信息，一般可以分为很多**记忆片段(memory segment)**，这些记忆片段按照一定的结构来进行组织。记忆片段一般用向量来表示，外部记忆单元可以用一组向量$$M = [\pmb m_1 , ⋯ ,\pmb m_N ]$$来表示。这些向量的组织方式可以是集合、树、栈或队列等。大部分信息存储于外部记
   忆中，不需要全时参与主网络的运算。
3. 读取模块$$R$$：根据主网络生成的查询向量$$\pmb q_r$$，从外部记忆单元中读取相应的信息$$\pmb r =R(\pmb M,\pmb q_r)$$。
4. 写入模块$$W$$：根据主网络生成的查询向量$$\pmb q_w$$和要写入的信息$$\pmb a$$来更新外部记忆$$M = W(M,\pmb q_w, \pmb a)$$。

这种结构化的外部记忆是带有地址的，即每个记忆片段都可以按地址读取和写入。要实现类似于人脑神经网络的联想记忆能力，就需要按内容寻址的方式进行定位，然后进行读取或写入操作。按内容寻址通常使用注意力机制来进行，通过注意力机制可以实现一种 “软性” 的寻址方式，即计算一个在所有记忆片段上的分布，而不是一个单一的绝对地址。比如读取模型$$R$$的实现方式可以为
$$
\pmb r=\sum_{n=1}^N\alpha_n\pmb m_n\\
\alpha_n={\rm softmax}(s(\pmb m_n,\pmb q_r))
$$
其中$$\pmb q_r$$是主网络生成的查询向量，$$s(⋅, ⋅)$$为打分函数。类比于计算机的存储器读取，计算注意力分布的过程相当于是计算机的 “寻址” 过程，信息加权平均的过程相当于计算机的 “内容读取” 过程。因此，结构化的外部记忆也是一种联想记忆，只是其结构以及读写的操作方式更像是受计算机架构的启发。

通过引入外部记忆，可以将神经网络的参数和记忆容量 “分离”，即在少量增加网络参数的条件下可以大幅增加网络容量。因此，我们可以将注意力机制看作一个接口，将信息的存储与计算分离。

外部记忆从记忆结构、读写方式等方面可以演变出很多模型，比较典型的结构化外部记忆模型包括端到端记忆网络、神经图灵机等。



## 端到端记忆网络

**端到端记忆网络(End-To-End Memory Network, MemN2N)** [Sukhbaatar et al., 2015] 采用一种可微的网络结构，可以多次从外部记忆中读取信息。在端到端记忆网络中，外部记忆单元是只读的。

给定一组需要存储的信息$$\pmb m_{1∶N} = \{\pmb m_1 , ⋯ ,\pmb m_N\}$$，首先将其转换成两组记忆片段$$A = [\pmb a_1 , ⋯ ,\pmb a_N ]$$和$$C = [\pmb c_1 , ⋯ ,\pmb c_N]$$，分别存放在两个外部记忆单元中，其中$$A$$用来进行寻址，$$C$$用来进行输出。

主网络根据输入$$\pmb x$$生成$$\pmb q$$ ，并使用键值对注意力机制来从外部记忆中读取相关信息$$\pmb r$$，
$$
\pmb r=\sum_{n=1}^N {\rm softmax}(\pmb a_n^{\rm T}\pmb q)\pmb c_n\\
$$
并产生输出
$$
\pmb y = f(\pmb q +\pmb r)
$$

其中$$f(⋅)$$为预测函数，当应用到分类任务时，$$f(⋅)$$可以设为 Softmax 函数。

**多跳操作**

为了实现更复杂的计算，我们可以让主网络和外部记忆进行多轮交互。在第$$k$$轮交互中，主网络根据上次从外部记忆中读取的信息$$\pmb r^{(k−1)}$$，产生新的查询向量
$$
\pmb q^{(k)}=\pmb r^{(k-1)}+\pmb q^{(k-1)}
$$
其中$$\pmb q^{(0)}$$为初始的查询向量，$$\pmb r^{(0)}=0$$。

假设第$$k$$轮交互的外部记忆为$$A^{(k)}$$和$$C^{(k)}$$，主网络从外部记忆读取信息为
$$
\pmb r^{(k)}=\sum_{n=1}^N {\rm softmax}((\pmb a_n^{(k)})^{\rm T}\pmb q^{(k)})\pmb c_n^{(k)}\\
$$
在$$K$$轮交互后，用$$\pmb y = f(\pmb q^{(K)} +\pmb r^{(K)})$$进行预测。这种多轮的交互方式也称为**多跳(multi-hop)**操作。多跳操作中的参数一般是共享的，为了简化起见，每轮交互的外部记忆也可以共享使用，比如$$A^{(1)} = ⋯ = A^{(K)}$$和$$C^{(1)} = ⋯ = C^{(K)}$$。

端到端记忆网络结构如下图所示。

![](https://i.loli.net/2020/11/10/kBxe9ME6FWHDJZI.png)



## 神经图灵机

**图灵机(Turing machine)**是图灵在 1936 年提出的一种抽象数学模型，可以用来模拟任何可计算问题 [Turing, 1937] 。如图所示，图灵机是一种自动机，它采用**带（tape）**作为临时存储。带可以被划分为若干单元，其中每一个单元内包含一个符号，图灵机的**读写头（read-write head）**能够在带上左右移动，每次移动能够读写一个符号。

![Screenshot from 2020-09-14 13-41-54.png](https://i.loli.net/2020/09/14/HD5Z4kW8dNgj3a9.png)

> **定义** 图灵机$$M$$由一个七元组定义
> $$
> M=(Q,\Sigma,\Gamma,\delta,q_0,\square,F)
> $$
> 其中
>
> + $$Q$$是控制部件内部状态的有穷集
>
> + $$\Sigma$$是输入字母表
>
> + $$\Gamma$$是要给有穷符号集，称为**带字母表（tape alphabet）**
>
> + $$\delta:Q\times \Gamma \to Q\times \Gamma \times \{L,R\}$$的有穷子集，称为状态转移函数
>
> + $$q_0\in Q$$是控制部件的初始状态
>
> + $$\square\in \Gamma$$是**空白符（blank）**
>
> + $$F\sube Q$$是终止状态集合
>
> @考虑图灵机
> $$
> Q=\{q_0,q_1\}\\
>   \Sigma=\{a,b\}\\
>   \Gamma=\{a,b,\square\}\\
>   F=\{q_1\}\\
>   \delta(q_0,a)=\{q_0,b,R\}\\
>   \delta(q_0,b)=\{q_0,b,R\}\\
>   \delta(q_0,\square)=\{q_1,\square,L\}\\
> $$
> 如图所示，如果该图灵机处于初态$$q_0$$并且读写头指向符号$$a$$，则读写头将$$a$$写为$$b$$，然后向右移动；如果指向符号$$b$$，则直接向右移动；如果指向空白符，则向左移动一个单元，并状态转移为终态$$q_1$$。
>
>   ![Screenshot from 2020-09-14 13-55-23.png](https://i.loli.net/2020/09/14/UNDJT7sKCqVQwYn.png)
>



**神经图灵机(Neural Turing Machine, NTM)** [Graves et al., 2014] 主要由两个部件构成：控制器和外部记忆。外部记忆定义为矩阵$$M ∈\mathbb{R}^{D×N}$$，这里$$N$$是记忆片段的数量，$$D$$是每个记忆片段的大小；控制器为一个前馈或循环神经网络。神经图灵机中的外部记忆是可读写的，其结构如下图所示。

![](https://i.loli.net/2020/11/10/4e8FTcMqbZaUzg1.png)

在每个时刻$$t$$，控制器接受当前时刻的输入$$\pmb x_t$$ 、上一时刻的输出$$\pmb h_{t−1}$$和上一时刻从外部记忆中读取的信息$$\pmb r_{t−1}$$，并产生输出$$\pmb h_t$$，同时生成和读写外部记忆相关的三个向量：查询向量$$\pmb q_t$$、删除向量$$\pmb e_t$$和增加向量$$\pmb a_t$$，然后对外部记忆$$M_t$$进行读写操作，生成读向量$$\pmb r_t$$和新的外部记忆$$M_{t+1}$$。

在$$t$$时刻，外部记忆的内容记为$$M_t = [\pmb m_{t,1} , ⋯ ,\pmb m_{t,N}]$$，**读操作**为从外部记忆$$M_t$$中读取信息$$\pmb r_t ∈ \mathbb{R}^{D}$$。首先通过注意力机制来进行基于内容的寻址，即
$$
\alpha_{t,n}={\rm softmax}(s(\pmb m_{t,n},\pmb q_t))
$$
其中$$\pmb q_t$$为控制器产生的查询向量，函数$$s(⋅, ⋅)$$为加性或乘性的打分函数。根据注意力分布$$\alpha_t$$，可以计算读向量(read vector) $$\pmb r_t$$作为下一个时刻控制器的输入。
$$
\pmb r_t=\sum_{n=1}^N \alpha_{t,n}\pmb m_{t,n}
$$
外部记忆的**写操作**可以分解为两个子操作：删除和增加。

首先，控制器产生**删除向量(erase vector)** $$\pmb e_t$$和**增加向量(add vector)**$$\pmb a_t$$，分别为要从外部记忆中删除的信息和要增加的信息。删除操作是根据注意力分布来按比例地在每个记忆片段中删除$$\pmb e_t$$，增加操作是根据注意力分布来按比例地给每个记忆片段加入$$\pmb a_t$$。具体过程如下：
$$
\pmb m_{t+1,n}=\pmb m_{t,n}(1-\alpha_{t,n}\pmb e_{t})+\alpha_{t,n}\pmb a_{t},\ \forall n\in[1,N]
$$
通过写操作得到下一时刻的外部记忆$$\pmb M_{t+1}$$。





# *基于神经动力学的联想记忆

……