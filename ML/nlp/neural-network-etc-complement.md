逻辑回归、神经网络、循环神经网络在NLP上的一些补充。



# 逻辑回归

朴素贝叶斯和逻辑回归的最大区别在于前者是一个**生成型(generative)**分类器，而后者是一个**区分型(discriminative)**分类器。

建立一个机器学习模型有两种非常不同的框架。打一个视觉上的比方，假设我们要区分狗的图片和猫的图片，生成型模型的思路就是让模型理解狗的模样和猫的模样，当给出一张测试图片时，模型会检查是狗的模样还是猫的模样更匹配它，因而给出标签；区分型模型的思路则是学习狗和猫区别，至于狗和猫本身的模样并不重要。

更正式地，回想贝叶斯方法中将文本$$d$$划分到类别$$c$$的条件概率$$P(c|d)$$通过先验和似然计算：
$$
\hat{c}=\arg\max_{c\in C}P(d|c)P(c)\\
\quad\quad\quad\quad\quad\quad似然\ \ 先验
$$
生成型模型就是利用似然这一项，它表示如果类型是$$c$$，那么文本$$d$$应该有哪些特征，实际上又是不是有这些特征。

与之相对地，区分型模型在文本分类场景尝试的是直接计算$$P(c|d)$$。它会为那些能够区分不同类别的特征学习到高的权重，尽管它并不能为某个类别生成一个实例。



逻辑回归相比朴素贝叶斯有一些优势：朴素贝叶斯有很强的条件独立的假定，但是各条件很可能相关；逻辑回归对于相关特征的鲁棒性也更强，因为它可以将权重拆分到这几个特征上。因此当特征相关时，逻辑回归预测的概率会比朴素贝叶斯更准确，因此在大型的数据集上普遍表现更好，是一个默认的选项。

尽管如此，<u>朴素贝叶斯方法在小型数据集和短文本上依然表现很好</u>，并且<u>易于实现、训练迅速</u>，因此在某些情况下也是合理的选择。





在许多情况下，我们不仅想要正确的分类，还想知道分类器为何做出了这样的选择，也就是让模型**可解释(interpretable)**。例如在逻辑回归模型中，一种理解分类器的决策的方法是理解每一个特征做出了多大的贡献。检查某个特定的特征是否重要可以通过统计检验方法，或者查看它的权重值，这能够帮助我们解释分类器的决策。





# 神经网络

## 神经语言模型

基于神经网络的语言模型相比n-gram模型有诸多优势，例如神经语言模型无需平滑……对于给定规模的训练集，神经语言模型能得到比n-gram语言模型高得多的预测准确率；并且神经语言模型还是机器翻译、对话和文本生成等任务的基础。

另一方面，表现提升的代价是训练相比传统模型慢得多，因此对于许多简单任务而言传统模型依然是合理的选择。

这里我们将介绍简单的前馈神经语言模型，最早由Bengio et al. (2003)提出。前馈神经语言模型是一个标准的前馈网络，将下一个词作为之前若干个词$$(w_{t-1},w_{t-2},\cdots)$$的表示并且返回一个可能的词的概率分布，就像n-gram那样：
$$
P(w_t|w_1^{t-1})\approx P(w_t|w_{t-N+1}^{t-1})
$$
在下面的例子中我们将使用4-gram模型实例。



### 嵌入





