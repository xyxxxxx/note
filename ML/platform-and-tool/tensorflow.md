# tensor

## create

```python
# scalar
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
# tf.Tensor(4, shape=(), dtype=int32)

# vector
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
# tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)

# matrix
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
# tf.Tensor(
# [[1. 2.]
#  [3. 4.]
#  [5. 6.]], shape=(3, 2), dtype=float16)

# rank 3 tensor
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
print(rank_3_tensor)
# tf.Tensor(
# [[[ 0  1  2  3  4]
#   [ 5  6  7  8  9]]

#  [[10 11 12 13 14]
#   [15 16 17 18 19]]

#  [[20 21 22 23 24]
#   [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)

# all ones tensor
ones = tf.ones([2,2])

# all zeros tensor
zeros = tf.zeros([2,2])

# sequence
sqc = tf.range(1,5) # [1 2 3 4]


```



## random

```python
# normal distribution
normal = tf.random.normal([2,2])
# tf.random.normal(shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None)

# uniform distribution
uni_float = tf.random.uniform([2,2],0,10)
uni_int = tf.random.uniform([2,2],0,10,tf.dtypes.int32)
# tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None)
```



## convert from & to NumPy array

```python
# np 2 tf
tf_tensor = tf.constant(np_array)

# tf 2 np
np_array = np.array(tf_tensor)
# or
np_array = tf_tensor.numpy()
```



## shape

```python
rank_4_tensor = tf.zeros([3, 2, 4, 5])
Math  				
æœª
å®š
ä¹‰
çš„
çŠ¶
æ€
è½¬
ç§»
å‡½
æ•°
è½¬
ç§»
è‡³
å¼€
å§‹
è¿
è¡Œ
ç»“
æŸ
é”™
è¯¯
# show shape
print("Type of every element:", rank_4_tensor.dtype)   # <dtype: 'float32'>
print("Number of dimensions:", rank_4_tensor.ndim)     # 4
print("Shape of tensor:", rank_4_tensor.shape)         # (3,2,4,5)
print("Length of first axis:", rank_4_tensor.shape[0]) # 3
print("Length of last axis:", rank_4_tensor.shape[-1]) # 5
print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy()) # 120

# manipulate shape
reshape = tf.reshape(rank_4_tensor,[4,5,6])
print(reshape)
# tf.Tensor(
# [[[0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]]

#  ...

#  [[0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]]], shape=(4, 5, 6), dtype=float32)

reshape = tf.reshape(rank_4_tensor,[10,-1]) # -1 fits length of last axis
print(reshape)
# tf.Tensor(
# [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  ...
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 12), dtype=float32)

reshape = tf.reshape(rank_4_tensor,[4,5,6])
reshape = tf.reshape(reshape,[7,-1])        # error
```



## operation

```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]])
print(a + b, "\n") # element-wise addition
# print(tf.add(a, b), "\n")
print(a * b, "\n") # element-wise multiplication
# print(tf.multiply(a, b), "\n")
print(a @ b, "\n") # matrix multiplication
# print(tf.matmul(a, b), "\n")
# tf.Tensor(
# [[2 3]
#  [4 5]], shape=(2, 2), dtype=int32) 

# tf.Tensor(
# [[1 2]
#  [3 4]], shape=(2, 2), dtype=int32) 

# tf.Tensor(
# [[3 3]
#  [7 7]], shape=(2, 2), dtype=int32) 


print(tf.reduce_max(a)) # max element
print(tf.argmax(a))     # index of max element
```

```python
x = tf.reshape(tf.range(1,4),[3,1])
y = tf.range(1,5)
print(tf.multiply(x,y))  # return 3*4 matrix
print(tf.multiply(y,x))  # return 3*4 matrix

print(tf.broadcast_to(x, [3, 3]))  # extend tensor
```



## index & slice

```python
# vector
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())

# matrix
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor[1, 1].numpy())
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")

rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
print(rank_3_tensor[:, :, 4])
# tf.Tensor(
# [[ 4  9]
#  [14 19]
#  [24 29]], shape=(3, 2), dtype=int32)
```



## type casting

```python
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
# tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
```



## ragged tensor

```python
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
# <tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>
print(ragged_tensor.shape)
# (4, None)
```



## string tensor

```python
scalar_of_string = tf.constant("Gray wolf")
print(scalar_of_string)
# tf.Tensor(b'Gray wolf', shape=(), dtype=string)
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
print(tensor_of_strings)
# tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)

unicode_string = tf.constant("ğŸ¥³ğŸ‘") # unicode string
print(unicode_string)
# <tf.Tensor: shape=(), dtype=string, numpy=b'\xf0\x9f\xa5\xb3\xf0\x9f\x91\x8d'>

print(tf.strings.split(scalar_of_string, sep=" "))  # split
# tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)
print(tf.strings.split(tensor_of_strings))          # split vector of strings
# <tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>

# ascii char
print(tf.strings.bytes_split(scalar_of_string))     # split to bytes
# tf.Tensor([b'G' b'r' b'a' b'y' b' ' b'w' b'o' b'l' b'f'], shape=(9,), dtype=string)
print(tf.io.decode_raw(scalar_of_string, tf.uint8)) # cast to ascii
# tf.Tensor([ 71 114  97 121  32 119 111 108 102], shape=(9,), dtype=uint8)

# unicode char
print(tf.strings.unicode_split(unicode_string, "UTF-8"))
print(tf.strings.unicode_decode(unicode_string, "UTF-8"))
# tf.Tensor([b'\xf0\x9f\xa5\xb3' b'\xf0\x9f\x91\x8d'], shape=(2,), dtype=string)
# tf.Tensor([129395 128077], shape=(2,), dtype=int32)
```



## sparse tensor

```python
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(tf.sparse.to_dense(sparse_tensor))
# tf.Tensor(
# [[1 0 0 0]
#  [0 0 2 0]
#  [0 0 0 0]], shape=(3, 4), dtype=int32)
```





# Variable

```python

```



## GradientTape

`tf.GradientTape()` æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ±‚å¯¼çš„è®°å½•å™¨ã€‚ä»¥ä¸‹ç¤ºä¾‹è®¡ç®—$$y=x^2$$åœ¨$$x=3$$ä½ç½®çš„å¯¼æ•°ï¼š

```python
import tensorflow as tf

x = tf.Variable(initial_value=3.)   # åˆå€¼ä¸º3.0çš„å˜é‡
with tf.GradientTape() as tape:     # åœ¨ tf.GradientTape() çš„ä¸Šä¸‹æ–‡å†…ï¼Œæ‰€æœ‰è®¡ç®—æ­¥éª¤éƒ½ä¼šè¢«è®°å½•ä»¥ç”¨äºæ±‚å¯¼
    y = tf.square(x)
y_grad = tape.gradient(y, x)        # è®¡ç®—yå…³äºxçš„å¯¼æ•°
print(y, y_grad)                    # tf.Tensor(6.0, shape=(), dtype=float32)
```

ä»¥ä¸‹ç¤ºä¾‹è®¡ç®—$$\mathcal{L}=||X\pmb w+b-\pmb y||^2$$åœ¨$$\pmb w=[1,2]^{\rm T},b=1$$ä½ç½®çš„å¯¹$$\pmb w,b$$çš„å¯¼æ•°ï¼š

```python
X = tf.constant([[1., 2.], [3., 4.]])
y = tf.constant([[1.], [2.]])
w = tf.Variable(initial_value=[[1.], [2.]])
b = tf.Variable(initial_value=1.)
with tf.GradientTape() as tape:
    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))
    # tf.square å°†è¾“å…¥å¼ é‡çš„æ¯ä¸ªå…ƒç´ å¹³æ–¹
    # tf.reduce_sum å¯¹è¾“å…¥å¼ é‡çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œ,è¾“å‡ºä¸€ä¸ªæ ‡é‡
w_grad, b_grad = tape.gradient(L, [w, b])        # è®¡ç®—L(w, b)å…³äºw, bçš„åå¯¼æ•°

print(L, w_grad, b_grad)
# tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(
# [[ 70.]
#  [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)
```

å¯ä»¥çœ‹åˆ°è®¡ç®—ç»“æœ
$$
\mathcal{L}=125,\ \frac{\partial \mathcal{L}}{\partial \pmb w}=\begin{bmatrix}70\\100\end{bmatrix},\ \frac{\partial \mathcal{L}}{\partial b}=30
$$






# keras

åœ¨ TensorFlow ä¸­ï¼Œæ¨èä½¿ç”¨ Kerasï¼ˆ `tf.keras` ï¼‰æ„å»ºæ¨¡å‹ã€‚Keras æ˜¯ä¸€ä¸ªå¹¿ä¸ºæµè¡Œçš„é«˜çº§ç¥ç»ç½‘ç»œ APIï¼Œç®€å•ã€å¿«é€Ÿè€Œä¸å¤±çµæ´»æ€§ï¼Œç°å·²å¾—åˆ° TensorFlow çš„å®˜æ–¹å†…ç½®å’Œå…¨é¢æ”¯æŒã€‚

keras æœ‰ä¸¤ä¸ªé‡è¦çš„æ¦‚å¿µï¼š **æ¨¡å‹ï¼ˆmodelï¼‰** å’Œ **å±‚ï¼ˆlayerï¼‰** ã€‚å±‚å°†å„ç§è®¡ç®—æµç¨‹å’Œå˜é‡è¿›è¡Œäº†å°è£…ï¼ˆä¾‹å¦‚åŸºæœ¬çš„å…¨è¿æ¥å±‚ï¼ŒCNN çš„å·ç§¯å±‚ã€æ± åŒ–å±‚ç­‰ï¼‰ï¼Œè€Œæ¨¡å‹åˆ™å°†å„ç§å±‚è¿›è¡Œç»„ç»‡å’Œè¿æ¥ï¼Œå¹¶å°è£…æˆä¸€ä¸ªæ•´ä½“ï¼Œæè¿°äº†å¦‚ä½•å°†è¾“å…¥æ•°æ®é€šè¿‡å„ç§å±‚ä»¥åŠè¿ç®—è€Œå¾—åˆ°è¾“å‡ºã€‚



## layer

`tf.keras.layers` ä¸‹å†…ç½®äº†æ·±åº¦å­¦ä¹ ä¸­å¤§é‡å¸¸ç”¨çš„çš„é¢„å®šä¹‰å±‚ï¼ŒåŒæ—¶ä¹Ÿå…è®¸æˆ‘ä»¬è‡ªå®šä¹‰å±‚ã€‚

### Dense

å…¨è¿æ¥å±‚ï¼ˆfully-connected layerï¼Œ`tf.keras.layers.Dense` ï¼‰æ˜¯ Keras ä¸­æœ€åŸºç¡€å’Œå¸¸ç”¨çš„å±‚ä¹‹ä¸€ï¼Œå¯¹è¾“å…¥çŸ©é˜µ $$A$$ è¿›è¡Œ $$f(A\pmb w+b)$$ çš„çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°æ“ä½œã€‚å¦‚æœä¸æŒ‡å®šæ¿€æ´»å‡½æ•°ï¼Œå³æ˜¯çº¯ç²¹çš„çº¿æ€§å˜æ¢ $$A\pmb w+b$$ã€‚å…·ä½“è€Œè¨€ï¼Œç»™å®šè¾“å…¥å¼ é‡ `input = [batch_size, input_dim]` ï¼Œè¯¥å±‚å¯¹è¾“å…¥å¼ é‡é¦–å…ˆè¿›è¡Œ `tf.matmul(input, kernel) + bias` çš„çº¿æ€§å˜æ¢ï¼ˆ `kernel` å’Œ `bias` æ˜¯å±‚ä¸­å¯è®­ç»ƒçš„å˜é‡ï¼‰ï¼Œç„¶åå¯¹çº¿æ€§å˜æ¢åå¼ é‡çš„æ¯ä¸ªå…ƒç´ é€šè¿‡æ¿€æ´»å‡½æ•° `activation` ï¼Œä»è€Œè¾“å‡ºå½¢çŠ¶ä¸º `[batch_size, units]` çš„äºŒç»´å¼ é‡ã€‚

[![../../_images/dense.png](https://tf.wiki/_images/dense.png)](https://tf.wiki/_images/dense.png)

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `units` ï¼šç¥ç»å…ƒçš„ä¸ªæ•°ï¼Œä¹Ÿæ˜¯è¾“å‡ºå¼ é‡çš„ç»´åº¦
+ `activation` ï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºæ— æ¿€æ´»å‡½æ•°ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ `tf.nn.relu` ã€ `tf.nn.tanh` å’Œ `tf.nn.sigmoid` 
+ `use_bias` ï¼šæ˜¯å¦åŠ å…¥åç½®å‘é‡ `bias` ï¼Œé»˜è®¤ä¸º `True` 
+ `kernel_initializer` ã€ `bias_initializer` ï¼šæƒé‡çŸ©é˜µ `kernel` å’Œåç½®å‘é‡ `bias` ä¸¤ä¸ªå˜é‡çš„åˆå§‹åŒ–å™¨ã€‚é»˜è®¤ä¸º `tf.glorot_uniform_initializer`ã€‚è®¾ç½®ä¸º `tf.zeros_initializer` è¡¨ç¤ºå°†ä¸¤ä¸ªå˜é‡å‡åˆå§‹åŒ–ä¸ºå…¨ 0

è¯¥å±‚åŒ…å«æƒé‡çŸ©é˜µ `kernel = [input_dim, units]` å’Œåç½®å‘é‡ `bias = [units]`ä¸¤ä¸ªå¯è®­ç»ƒå˜é‡ï¼Œå¯¹åº”äº $$f(A\pmb w+b)$$ ä¸­çš„ $$\pmb w$$ å’Œ $$b$$ã€‚



### Conv2D

å·ç§¯å±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `filters`ï¼šè¾“å‡ºç‰¹å¾æ˜ å°„çš„ä¸ªæ•°
+ `kernel_size`ï¼šæ•´æ•°æˆ–æ•´æ•°1Ã—2å‘é‡ï¼Œï¼ˆåˆ†åˆ«ï¼‰è¡¨ç¤ºäºŒç»´å·ç§¯æ ¸çš„é«˜å’Œå®½
+ `strides`ï¼šæ•´æ•°æˆ–æ•´æ•°1Ã—2å‘é‡ï¼Œï¼ˆåˆ†åˆ«ï¼‰è¡¨ç¤ºå·ç§¯çš„çºµå‘å’Œæ¨ªå‘æ­¥é•¿
+ `padding`ï¼š`"valid"`è¡¨ç¤ºå¯¹äºä¸å¤Ÿå·ç§¯æ ¸å¤§å°çš„éƒ¨åˆ†ä¸¢å¼ƒï¼Œ`"same"`è¡¨ç¤ºå¯¹äºä¸å¤Ÿå·ç§¯æ ¸å¤§å°çš„éƒ¨åˆ†è¡¥0ï¼Œé»˜è®¤ä¸º`"valid"`
+ `activation`ï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºæ— æ¿€æ´»å‡½æ•°
+ `use_bias`ï¼šæ˜¯å¦ä½¿ç”¨åç½®ï¼Œé»˜è®¤ä¸ºä½¿ç”¨

ç¤ºä¾‹ï¼š

```python
model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
# è¾“å…¥32x32RGBå›¾ç‰‡,è¾“å‡º32ä¸ªç‰¹å¾æ˜ å°„,ä½¿ç”¨3x3å·ç§¯æ ¸,æ¯ä¸ªè¾“å‡ºç‰¹å¾æ˜ å°„ä½¿ç”¨1ä¸ªåç½®
# å‚æ•°æ•°é‡ä¸º3x32x(3x3)+32=896
model.add(keras.layers.MaxPooling2D((2, 2)))
# å¯¹æ¯ä¸ª2x2åŒºå—æ‰§è¡Œæœ€å¤§æ±‡èš
model.add(keras.layers.Conv2D(64, (3, 3), (2, 2), activation='relu'))
# å·ç§¯çš„æ­¥é•¿è®¾ä¸º2
model.add(keras.layers.MaxPooling2D((2, 2)))
# 7%2=1,å› æ­¤ä¸¢å¼ƒä¸€è¡Œä¸€åˆ—çš„æ•°æ®
model.summary()

# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# conv2d (Conv2D)              (None, 30, 30, 32)        896       
# _________________________________________________________________
# max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
# _________________________________________________________________
# conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     
# _________________________________________________________________
# max_pooling2d_1 (MaxPooling2 (None, 3, 3, 64)          0         
# =================================================================
# Total params: 19,392
# Trainable params: 19,392
# Non-trainable params: 0
```



### MaxPooling2D

æ±‡èšå±‚ï¼ˆæ± åŒ–å±‚ï¼‰ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `pool_size`ï¼šæœ€å¤§æ±‡èšçš„åŒºåŸŸè§„æ¨¡ï¼Œé»˜è®¤ä¸º`(2,2)`
+ `strides`ï¼šæœ€å¤§æ±‡èšçš„æ­¥é•¿ï¼Œé»˜è®¤ä¸º`None`
+ `padding`ï¼š`"valid"`è¡¨ç¤ºå¯¹äºä¸å¤ŸåŒºåŸŸå¤§å°çš„éƒ¨åˆ†ä¸¢å¼ƒï¼Œ`"same"`è¡¨ç¤ºå¯¹äºä¸å¤ŸåŒºåŸŸå¤§å°çš„éƒ¨åˆ†è¡¥0ï¼Œé»˜è®¤ä¸º`"valid"`



### Embedding

> å‚è€ƒ[å•è¯åµŒå…¥å‘é‡](https://www.tensorflow.org/tutorials/text/word_embeddings)

åµŒå…¥å±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `input_dim`ï¼šå­—å…¸çš„è§„æ¨¡
+ `output_dim`ï¼šåµŒå…¥å‘é‡çš„è§„æ¨¡
+ `mask_zero`ï¼šæ˜¯å¦å°†è¾“å…¥ä¸­çš„0çœ‹ä½œå¡«å……å€¼è€Œå¿½ç•¥ä¹‹ï¼Œé»˜è®¤ä¸º`False`
+ `input_length`ï¼šè¾“å…¥åºåˆ—çš„é•¿åº¦ï¼ˆå¦‚æœè¯¥é•¿åº¦å›ºå®šï¼‰ï¼Œé»˜è®¤ä¸º`None`ï¼›å¦‚æœæ­¤åµŒå…¥å±‚åæ¥`Flatten`å±‚ï¼Œå†æ¥`Dense`å±‚ï¼Œåˆ™å¿…é¡»åˆ¶å®šæ­¤å‚æ•°

ç¤ºä¾‹è§LSTMã€‚



### LSTM

LSTMå±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `units`ï¼šè¾“å‡ºç©ºé—´çš„è§„æ¨¡



ç¤ºä¾‹ï¼š

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 16))
# å°†è§„æ¨¡ä¸º10000çš„è¯å…¸åµŒå…¥åˆ°16ç»´å‘é‡
# è¾“å…¥é•¿åº¦ä¸º256çš„å‘é‡,è¾“å‡ºè§„æ¨¡ä¸º256x16çš„å¼ é‡
model.add(tf.keras.layers.LSTM(64))
# LSTMå±‚
# è¾“å…¥è§„æ¨¡ä¸º256x16çš„å¼ é‡,è¾“å‡ºé•¿åº¦ä¸º64çš„å‘é‡,ç›¸å½“äº(åŒæ­¥æˆ–å¼‚æ­¥)åºåˆ—åˆ°åºåˆ—æ¨¡å¼æ¯4ä¸ªè¾“å…¥å‘é‡è¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼
model.add(keras.layers.Dense(16, activation='relu'))
# å…¨è¿æ¥å±‚,ReLUæ¿€æ´»å‡½æ•°,åˆ†ç±»å™¨
model.add(keras.layers.Dense(1, activation='sigmoid'))  
# å…¨è¿æ¥å±‚,Logisticæ¿€æ´»å‡½æ•°
model.summary()
```





### Bidirectional







## model

### Sequential

`Sequential`è¿”å›ä¸€ä¸ª`keras.Model`å¯¹è±¡ã€‚`Sequential`æ¨¡å‹é€‚ç”¨äºFNNï¼ŒCNNï¼ŒRNNç­‰ï¼Œå…¶ä¸­æ¯ä¸€å±‚éƒ½æœ‰**ä¸€ä¸ªè¾“å…¥å¼ é‡å’Œä¸€ä¸ªè¾“å‡ºå¼ é‡** ã€‚

ä»¥ä¸‹`Sequential`æ¨¡å‹ï¼Œ

```python
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu", name="layer1"),
        layers.Dense(3, activation="relu", name="layer2"),
        layers.Dense(4, name="layer3"),
    ]
)
# Call model on a test input
x = tf.ones((3, 3))
y = model(x)
```

ç­‰æ•ˆäºä»¥ä¸‹åŠŸèƒ½

```python
# Create 3 layers
layer1 = layers.Dense(2, activation="relu", name="layer1")
layer2 = layers.Dense(3, activation="relu", name="layer2")
layer3 = layers.Dense(4, name="layer3")

# Call layers on a test input
x = tf.ones((3, 3))
y = layer3(layer2(layer1(x)))
```

`Sequential`æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨`add()`æ–¹æ³•åˆ›å»º

```python
model = keras.Sequential()
model.add(layers.Dense(2, activation="relu"))
model.add(layers.Dense(3, activation="relu"))
model.add(layers.Dense(4))
```

ä¸€æ—¦åˆ›å»ºäº†æ¨¡å‹ï¼Œå°±å¯ä»¥è°ƒç”¨`summary()`æ–¹æ³•æ˜¾ç¤ºå…¶å†…å®¹ã€‚



CNNæ¨¡å‹ç¤ºä¾‹

```python
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))

model.add(layers.Dense(10))
```



### compile





### fit





### evaluate





### è‡ªå®šä¹‰æ¨¡å‹

Keras æ¨¡å‹ä»¥ç±»çš„å½¢å¼å‘ˆç°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿ `tf.keras.Model` è¿™ä¸ª Python ç±»æ¥å®šä¹‰è‡ªå·±çš„æ¨¡å‹ã€‚åœ¨ç»§æ‰¿ç±»ä¸­ï¼Œæˆ‘ä»¬éœ€è¦é‡å†™ `__init__()` ï¼ˆæ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–ï¼‰å’Œ `call(input)` ï¼ˆæ¨¡å‹è°ƒç”¨ï¼‰ä¸¤ä¸ªæ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¢åŠ è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚

```python
class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()     # Python 2 ä¸‹ä½¿ç”¨ super(MyModel, self).__init__()
        # æ­¤å¤„æ·»åŠ åˆå§‹åŒ–ä»£ç ï¼ˆåŒ…å« call æ–¹æ³•ä¸­ä¼šç”¨åˆ°çš„å±‚ï¼‰ï¼Œä¾‹å¦‚
        # layer1 = tf.keras.layers.BuiltInLayer(...)
        # layer2 = MyCustomLayer(...)

    def call(self, input):
        # æ­¤å¤„æ·»åŠ æ¨¡å‹è°ƒç”¨çš„ä»£ç ï¼ˆå¤„ç†è¾“å…¥å¹¶è¿”å›è¾“å‡ºï¼‰ï¼Œä¾‹å¦‚
        # x = layer1(input)
        # output = layer2(x)
        return output

    # è¿˜å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ–¹æ³•
```

ä»¥ä¸‹ä¸ºè‡ªå®šä¹‰çº¿æ€§æ¨¡å‹ç¤ºä¾‹ï¼š

```python
class Linear(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense = tf.keras.layers.Dense(
            units=1,
            activation=None,
            kernel_initializer=tf.zeros_initializer(),
            bias_initializer=tf.zeros_initializer()
        )

    def call(self, input):
        output = self.dense(input)
        return output
```







# visualize

## example: gray image

```python
# draw image
plt.figure()
plt.imshow(train_images[0], cmap=plt.cm.binary)
plt.colorbar()
plt.grid(False)
plt.show()

# draw multiple images
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()
```



## example: line chart

```python
def plot_history(history):
  hist = pd.DataFrame(history.history)
  hist['epoch'] = history.epoch
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [MPG]')
  plt.plot(hist['epoch'], hist['mae'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mae'],
           label = 'Val Error')
  plt.ylim([0,5])
  plt.legend()
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Square Error [$MPG^2$]')
  plt.plot(hist['epoch'], hist['mse'],
           label='Train Error')
  plt.plot(hist['epoch'], hist['val_mse'],
           label = 'Val Error')
  plt.ylim([0,20])
  plt.legend()
  plt.show()
```

