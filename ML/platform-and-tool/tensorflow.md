[toc]

# tensor

## create

```python
# scalar
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)
# tf.Tensor(4, shape=(), dtype=int32)

# vector
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)
# tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)

# matrix
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)
# tf.Tensor(
# [[1. 2.]
#  [3. 4.]
#  [5. 6.]], shape=(3, 2), dtype=float16)

# rank 3 tensor
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
print(rank_3_tensor)
# tf.Tensor(
# [[[ 0  1  2  3  4]
#   [ 5  6  7  8  9]]

#  [[10 11 12 13 14]
#   [15 16 17 18 19]]

#  [[20 21 22 23 24]
#   [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)

# all ones tensor
ones = tf.ones([2,2])

# all zeros tensor
zeros = tf.zeros([2,2])

# sequence
sqc = tf.range(1,5) # [1 2 3 4]


```



## random

```python
# normal distribution
normal = tf.random.normal([2,2])
# tf.random.normal(shape, mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None)

# uniform distribution
uni_float = tf.random.uniform([2,2],0,10)
uni_int = tf.random.uniform([2,2],0,10,tf.dtypes.int32)
# tf.random.uniform(shape, minval=0, maxval=None, dtype=tf.dtypes.float32, seed=None)
```



## convert from & to NumPy array

```python
# np 2 tf
tf_tensor = tf.constant(np_array)

# tf 2 np
np_array = np.array(tf_tensor)
# or
np_array = tf_tensor.numpy()
```



## shape

```python
rank_4_tensor = tf.zeros([3, 2, 4, 5])

# show shape
print("Type of every element:", rank_4_tensor.dtype)   # <dtype: 'float32'>
print("Number of dimensions:", rank_4_tensor.ndim)     # 4
print("Shape of tensor:", rank_4_tensor.shape)         # (3,2,4,5)
print("Length of first axis:", rank_4_tensor.shape[0]) # 3
print("Length of last axis:", rank_4_tensor.shape[-1]) # 5
print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy()) # 120

# manipulate shape
reshape = tf.reshape(rank_4_tensor,[4,5,6])
print(reshape)
# tf.Tensor(
# [[[0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]]

#  ...

#  [[0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]
#   [0. 0. 0. 0. 0. 0.]]], shape=(4, 5, 6), dtype=float32)

reshape = tf.reshape(rank_4_tensor,[10,-1]) # -1 fits length of last axis
print(reshape)
# tf.Tensor(
# [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#  ...
#  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 12), dtype=float32)

reshape = tf.reshape(rank_4_tensor,[4,5,6])
reshape = tf.reshape(reshape,[7,-1])        # error
```



## operation

```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]])
print(a + b, "\n") # element-wise addition
# print(tf.add(a, b), "\n")
print(a * b, "\n") # element-wise multiplication
# print(tf.multiply(a, b), "\n")
print(a @ b, "\n") # matrix multiplication
# print(tf.matmul(a, b), "\n")
# tf.Tensor(
# [[2 3]
#  [4 5]], shape=(2, 2), dtype=int32) 

# tf.Tensor(
# [[1 2]
#  [3 4]], shape=(2, 2), dtype=int32) 

# tf.Tensor(
# [[3 3]
#  [7 7]], shape=(2, 2), dtype=int32) 


print(tf.reduce_max(a)) # max element
print(tf.argmax(a))     # index of max element
```

```python
x = tf.reshape(tf.range(1,4),[3,1])
y = tf.range(1,5)
print(tf.multiply(x,y))  # return 3*4 matrix
print(tf.multiply(y,x))  # return 3*4 matrix

print(tf.broadcast_to(x, [3, 3]))  # extend tensor
```



## index & slice

```python
# vector
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())

# matrix
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor[1, 1].numpy())
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")

rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
print(rank_3_tensor[:, :, 4])
# tf.Tensor(
# [[ 4  9]
#  [14 19]
#  [24 29]], shape=(3, 2), dtype=int32)
```



## type casting

```python
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
# tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
```



## ragged tensor

```python
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
# <tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>
print(ragged_tensor.shape)
# (4, None)
```



## string tensor

```python
scalar_of_string = tf.constant("Gray wolf")
print(scalar_of_string)
# tf.Tensor(b'Gray wolf', shape=(), dtype=string)
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
print(tensor_of_strings)
# tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)

unicode_string = tf.constant("ğŸ¥³ğŸ‘") # unicode string
print(unicode_string)
# <tf.Tensor: shape=(), dtype=string, numpy=b'\xf0\x9f\xa5\xb3\xf0\x9f\x91\x8d'>

print(tf.strings.split(scalar_of_string, sep=" "))  # split
# tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)
print(tf.strings.split(tensor_of_strings))          # split vector of strings
# <tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>

# ascii char
print(tf.strings.bytes_split(scalar_of_string))     # split to bytes
# tf.Tensor([b'G' b'r' b'a' b'y' b' ' b'w' b'o' b'l' b'f'], shape=(9,), dtype=string)
print(tf.io.decode_raw(scalar_of_string, tf.uint8)) # cast to ascii
# tf.Tensor([ 71 114  97 121  32 119 111 108 102], shape=(9,), dtype=uint8)

# unicode char
print(tf.strings.unicode_split(unicode_string, "UTF-8"))
print(tf.strings.unicode_decode(unicode_string, "UTF-8"))
# tf.Tensor([b'\xf0\x9f\xa5\xb3' b'\xf0\x9f\x91\x8d'], shape=(2,), dtype=string)
# tf.Tensor([129395 128077], shape=(2,), dtype=int32)
```



## sparse tensor

```python
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(tf.sparse.to_dense(sparse_tensor))
# tf.Tensor(
# [[1 0 0 0]
#  [0 0 2 0]
#  [0 0 0 0]], shape=(3, 4), dtype=int32)
```





# Variable

```python

```



## GradientTape

`tf.GradientTape()` æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ±‚å¯¼çš„è®°å½•å™¨ã€‚ä»¥ä¸‹ç¤ºä¾‹è®¡ç®—$$y=x^2$$åœ¨$$x=3$$ä½ç½®çš„å¯¼æ•°ï¼š

```python
import tensorflow as tf

x = tf.Variable(initial_value=3.)   # åˆå€¼ä¸º3.0çš„å˜é‡
with tf.GradientTape() as tape:     # åœ¨ tf.GradientTape() çš„ä¸Šä¸‹æ–‡å†…ï¼Œæ‰€æœ‰è®¡ç®—æ­¥éª¤éƒ½ä¼šè¢«è®°å½•ä»¥ç”¨äºæ±‚å¯¼
    y = tf.square(x)
y_grad = tape.gradient(y, x)        # è®¡ç®—yå…³äºxçš„å¯¼æ•°
print(y, y_grad)                    # tf.Tensor(6.0, shape=(), dtype=float32)
```

ä»¥ä¸‹ç¤ºä¾‹è®¡ç®—$$\mathcal{L}=||X\pmb w+b-\pmb y||^2$$åœ¨$$\pmb w=[1,2]^{\rm T},b=1$$ä½ç½®çš„å¯¹$$\pmb w,b$$çš„å¯¼æ•°ï¼š

```python
X = tf.constant([[1., 2.], [3., 4.]])
y = tf.constant([[1.], [2.]])
w = tf.Variable(initial_value=[[1.], [2.]])
b = tf.Variable(initial_value=1.)
with tf.GradientTape() as tape:
    L = tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))
    # tf.square å°†è¾“å…¥å¼ é‡çš„æ¯ä¸ªå…ƒç´ å¹³æ–¹
    # tf.reduce_sum å¯¹è¾“å…¥å¼ é‡çš„æ‰€æœ‰å…ƒç´ æ±‚å’Œ,è¾“å‡ºä¸€ä¸ªæ ‡é‡
w_grad, b_grad = tape.gradient(L, [w, b])        # è®¡ç®—L(w, b)å…³äºw, bçš„åå¯¼æ•°

print(L, w_grad, b_grad)
# tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(
# [[ 70.]
#  [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)
```

å¯ä»¥çœ‹åˆ°è®¡ç®—ç»“æœ
$$
\mathcal{L}=125,\ \frac{\partial \mathcal{L}}{\partial \pmb w}=\begin{bmatrix}70\\100\end{bmatrix},\ \frac{\partial \mathcal{L}}{\partial b}=30
$$






# keras

åœ¨ TensorFlow ä¸­ï¼Œæ¨èä½¿ç”¨ Kerasï¼ˆ `tf.keras` ï¼‰æ„å»ºæ¨¡å‹ã€‚Keras æ˜¯ä¸€ä¸ªå¹¿ä¸ºæµè¡Œçš„é«˜çº§ç¥ç»ç½‘ç»œ APIï¼Œç®€å•ã€å¿«é€Ÿè€Œä¸å¤±çµæ´»æ€§ï¼Œç°å·²å¾—åˆ° TensorFlow çš„å®˜æ–¹å†…ç½®å’Œå…¨é¢æ”¯æŒã€‚

Keras æä¾›äº†å®šä¹‰å’Œè®­ç»ƒä»»ä½•ç±»å‹çš„ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¾¿æ·æ–¹æ³•ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

+ å…è®¸ä»£ç åœ¨CPUæˆ–GPUä¸Šè¿è¡Œå¹¶ä¸”æ— ç¼åˆ‡æ¢
+ æä¾›ç”¨æˆ·å‹å¥½çš„ API ä»¥ä½¿å¾—èƒ½å¤Ÿå¿«é€Ÿå»ºæ¨¡
+ æä¾›å¯¹äº CNN (for CV)ï¼ŒRNN (for time series) çš„å†…ç½®æ”¯æŒ
+ æ”¯æŒä»»æ„ç±»å‹çš„ç½‘ç»œç»“æ„

keras æœ‰ä¸¤ä¸ªé‡è¦çš„æ¦‚å¿µï¼š **æ¨¡å‹ï¼ˆmodelï¼‰** å’Œ **å±‚ï¼ˆlayerï¼‰** ã€‚å±‚å°†å„ç§è®¡ç®—æµç¨‹å’Œå˜é‡è¿›è¡Œäº†å°è£…ï¼ˆä¾‹å¦‚åŸºæœ¬çš„å…¨è¿æ¥å±‚ï¼ŒCNN çš„å·ç§¯å±‚ã€æ± åŒ–å±‚ç­‰ï¼‰ï¼Œè€Œæ¨¡å‹åˆ™å°†å„ç§å±‚è¿›è¡Œç»„ç»‡å’Œè¿æ¥ï¼Œå¹¶å°è£…æˆä¸€ä¸ªæ•´ä½“ï¼Œæè¿°äº†å¦‚ä½•å°†è¾“å…¥æ•°æ®é€šè¿‡å„ç§å±‚ä»¥åŠè¿ç®—è€Œå¾—åˆ°è¾“å‡ºã€‚



## layer

å±‚æ˜¯è¿›è¡Œæ•°æ®å¤„ç†çš„æ¨¡å—ï¼Œå®ƒè¾“å…¥ä¸€ä¸ªå¼ é‡ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªå¼ é‡ã€‚å°½ç®¡æœ‰ä¸€äº›å±‚æ˜¯æ— çŠ¶æ€çš„ï¼Œæ›´å¤šçš„å±‚éƒ½æœ‰å…¶æƒé‡å¼ é‡ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ³•å­¦ä¹ ã€‚`tf.keras.layers` ä¸‹å†…ç½®äº†æ·±åº¦å­¦ä¹ ä¸­å¤§é‡å¸¸ç”¨çš„çš„é¢„å®šä¹‰å±‚ï¼ŒåŒæ—¶ä¹Ÿå…è®¸æˆ‘ä»¬è‡ªå®šä¹‰å±‚ã€‚

### Dense

å…¨è¿æ¥å±‚ï¼ˆdensely connected layer, fully connected layer, `tf.keras.layers.Dense` ï¼‰æ˜¯ Keras ä¸­æœ€åŸºç¡€å’Œå¸¸ç”¨çš„å±‚ä¹‹ä¸€ï¼Œå¯¹è¾“å…¥çŸ©é˜µ $$A$$ è¿›è¡Œ $$f(A\pmb w+b)$$ çš„çº¿æ€§å˜æ¢ + æ¿€æ´»å‡½æ•°æ“ä½œã€‚å¦‚æœä¸æŒ‡å®šæ¿€æ´»å‡½æ•°ï¼Œå³æ˜¯çº¯ç²¹çš„çº¿æ€§å˜æ¢ $$A\pmb w+b$$ã€‚å…·ä½“è€Œè¨€ï¼Œç»™å®šè¾“å…¥å¼ é‡ `input = [batch_size, input_dim]` ï¼Œè¯¥å±‚å¯¹è¾“å…¥å¼ é‡é¦–å…ˆè¿›è¡Œ `tf.matmul(input, kernel) + bias` çš„çº¿æ€§å˜æ¢ï¼ˆ `kernel` å’Œ `bias` æ˜¯å±‚ä¸­å¯è®­ç»ƒçš„å˜é‡ï¼‰ï¼Œç„¶åå¯¹çº¿æ€§å˜æ¢åå¼ é‡çš„æ¯ä¸ªå…ƒç´ é€šè¿‡æ¿€æ´»å‡½æ•° `activation` ï¼Œä»è€Œè¾“å‡ºå½¢çŠ¶ä¸º `[batch_size, units]` çš„äºŒç»´å¼ é‡ã€‚

[![../../_images/dense.png](https://tf.wiki/_images/dense.png)](https://tf.wiki/_images/dense.png)

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `units` ï¼šç¥ç»å…ƒçš„ä¸ªæ•°ï¼Œä¹Ÿæ˜¯è¾“å‡ºå¼ é‡çš„ç»´åº¦
+ `activation` ï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºæ— æ¿€æ´»å‡½æ•°ã€‚å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ `tf.nn.relu` ã€ `tf.nn.tanh` å’Œ `tf.nn.sigmoid` 
+ `use_bias` ï¼šæ˜¯å¦åŠ å…¥åç½®å‘é‡ `bias` ï¼Œé»˜è®¤ä¸º `True` 
+ `kernel_initializer` ã€ `bias_initializer` ï¼šæƒé‡çŸ©é˜µ `kernel` å’Œåç½®å‘é‡ `bias` ä¸¤ä¸ªå˜é‡çš„åˆå§‹åŒ–å™¨ã€‚é»˜è®¤ä¸º `tf.glorot_uniform_initializer`ã€‚è®¾ç½®ä¸º `tf.zeros_initializer` è¡¨ç¤ºå°†ä¸¤ä¸ªå˜é‡å‡åˆå§‹åŒ–ä¸ºå…¨ 0

è¯¥å±‚åŒ…å«æƒé‡çŸ©é˜µ `kernel = [input_dim, units]` å’Œåç½®å‘é‡ `bias = [units]`ä¸¤ä¸ªå¯è®­ç»ƒå˜é‡ï¼Œå¯¹åº”äº $$f(A\pmb w+b)$$ ä¸­çš„ $$\pmb w$$ å’Œ $$b$$ã€‚



### Conv2D

å·ç§¯å±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `filters`ï¼šè¾“å‡ºç‰¹å¾æ˜ å°„çš„ä¸ªæ•°
+ `kernel_size`ï¼šæ•´æ•°æˆ–æ•´æ•°1Ã—2å‘é‡ï¼Œï¼ˆåˆ†åˆ«ï¼‰è¡¨ç¤ºäºŒç»´å·ç§¯æ ¸çš„é«˜å’Œå®½
+ `strides`ï¼šæ•´æ•°æˆ–æ•´æ•°1Ã—2å‘é‡ï¼Œï¼ˆåˆ†åˆ«ï¼‰è¡¨ç¤ºå·ç§¯çš„çºµå‘å’Œæ¨ªå‘æ­¥é•¿
+ `padding`ï¼š`"valid"`è¡¨ç¤ºå¯¹äºä¸å¤Ÿå·ç§¯æ ¸å¤§å°çš„éƒ¨åˆ†ä¸¢å¼ƒï¼Œ`"same"`è¡¨ç¤ºå¯¹äºä¸å¤Ÿå·ç§¯æ ¸å¤§å°çš„éƒ¨åˆ†è¡¥0ï¼Œé»˜è®¤ä¸º`"valid"`
+ `activation`ï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºæ— æ¿€æ´»å‡½æ•°
+ `use_bias`ï¼šæ˜¯å¦ä½¿ç”¨åç½®ï¼Œé»˜è®¤ä¸ºä½¿ç”¨

ç¤ºä¾‹ï¼š

```python
model = keras.models.Sequential()
model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
# è¾“å…¥32x32RGBå›¾ç‰‡,è¾“å‡º32ä¸ªç‰¹å¾æ˜ å°„,ä½¿ç”¨3x3å·ç§¯æ ¸,æ¯ä¸ªè¾“å‡ºç‰¹å¾æ˜ å°„ä½¿ç”¨1ä¸ªåç½®
# å‚æ•°æ•°é‡ä¸º3x32x(3x3)+32=896
model.add(keras.layers.MaxPooling2D((2, 2)))
# å¯¹æ¯ä¸ª2x2åŒºå—æ‰§è¡Œæœ€å¤§æ±‡èš
model.add(keras.layers.Conv2D(64, (3, 3), (2, 2), activation='relu'))
# å·ç§¯çš„æ­¥é•¿è®¾ä¸º2
model.add(keras.layers.MaxPooling2D((2, 2)))
# 7%2=1,å› æ­¤ä¸¢å¼ƒä¸€è¡Œä¸€åˆ—çš„æ•°æ®
model.summary()

# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# conv2d (Conv2D)              (None, 30, 30, 32)        896       
# _________________________________________________________________
# max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
# _________________________________________________________________
# conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     
# _________________________________________________________________
# max_pooling2d_1 (MaxPooling2 (None, 3, 3, 64)          0         
# =================================================================
# Total params: 19,392
# Trainable params: 19,392
# Non-trainable params: 0
```



### MaxPooling2D

æ±‡èšå±‚ï¼ˆæ± åŒ–å±‚ï¼‰ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `pool_size`ï¼šæœ€å¤§æ±‡èšçš„åŒºåŸŸè§„æ¨¡ï¼Œé»˜è®¤ä¸º`(2,2)`
+ `strides`ï¼šæœ€å¤§æ±‡èšçš„æ­¥é•¿ï¼Œé»˜è®¤ä¸º`None`
+ `padding`ï¼š`"valid"`è¡¨ç¤ºå¯¹äºä¸å¤ŸåŒºåŸŸå¤§å°çš„éƒ¨åˆ†ä¸¢å¼ƒï¼Œ`"same"`è¡¨ç¤ºå¯¹äºä¸å¤ŸåŒºåŸŸå¤§å°çš„éƒ¨åˆ†è¡¥0ï¼Œé»˜è®¤ä¸º`"valid"`



### Embedding

> å‚è€ƒ[å•è¯åµŒå…¥å‘é‡](https://www.tensorflow.org/tutorials/text/word_embeddings)

åµŒå…¥å±‚å¯ä»¥è¢«ç†è§£ä¸ºæ•´æ•°ï¼ˆå•è¯ç´¢å¼•ï¼‰åˆ°å¯†é›†å‘é‡çš„æ˜ å°„ã€‚åµŒå…¥å±‚è¾“å…¥å½¢å¦‚`(samples, sequence_length)`çš„äºŒç»´æ•´æ•°å¼ é‡ï¼Œå› æ­¤æ‰€æœ‰çš„æ•´æ•°åºåˆ—éƒ½åº”å¡«å……æˆ–è£å‰ªåˆ°ç›¸åŒçš„é•¿åº¦ï¼›è¾“å‡ºå½¢å¦‚`(samples, sequence_ length, embedding_dimensionality)`çš„ä¸‰ç»´æµ®ç‚¹å¼ é‡ï¼Œå†è¾“å…¥ç»™ RNN å±‚å¤„ç†ã€‚

åµŒå…¥å±‚åœ¨åˆšåˆå§‹åŒ–æ—¶æ‰€æœ‰çš„æƒé‡å‚æ•°éƒ½æ˜¯éšæœºçš„ï¼Œå°±å¦‚åŒå…¶å®ƒçš„å±‚ä¸€æ ·ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿™äº›å‚æ•°ä¼šæ ¹æ®åå‘ä¼ æ’­ç®—æ³•é€æ¸æ›´æ–°ï¼ŒåµŒå…¥ç©ºé—´ä¼šé€æ¸æ˜¾ç°å‡ºæ›´å¤šç»“æ„ï¼ˆè¿™äº›ç»“æ„é€‚åº”äºå½“å‰çš„å…·ä½“é—®é¢˜ï¼‰ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `input_dim`ï¼šå­—å…¸çš„è§„æ¨¡
+ `output_dim`ï¼šåµŒå…¥å‘é‡çš„è§„æ¨¡
+ `mask_zero`ï¼šæ˜¯å¦å°†è¾“å…¥ä¸­çš„0çœ‹ä½œå¡«å……å€¼è€Œå¿½ç•¥ä¹‹ï¼Œé»˜è®¤ä¸º`False`
+ `input_length`ï¼šè¾“å…¥åºåˆ—çš„é•¿åº¦ï¼ˆå¦‚æœè¯¥é•¿åº¦å›ºå®šï¼‰ï¼Œé»˜è®¤ä¸º`None`ï¼›å¦‚æœæ­¤åµŒå…¥å±‚åæ¥`Flatten`å±‚ï¼Œå†æ¥`Dense`å±‚ï¼Œåˆ™å¿…é¡»åˆ¶å®šæ­¤å‚æ•°

ç¤ºä¾‹è§SimpleRNNï¼ŒLSTMã€‚



### SimpleRNN

SRNå±‚æ˜¯æœ€ç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œå±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `units`ï¼šè¾“å‡ºå‘é‡çš„ç»´åº¦
+ `activation`ï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸º`tanh`
+ `return_sequences`ï¼š`False`è¡¨ç¤ºæœ€åè¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œå³åºåˆ—åˆ°ç±»åˆ«æ¨¡å¼ï¼›`True`è¡¨ç¤ºæ¯ä¸ªæ—¶é—´æ­¥é•¿è¾“å‡ºä¸€ä¸ªå‘é‡ï¼Œå³åºåˆ—åˆ°åºåˆ—æ¨¡å¼ã€‚

å®è·µä¸­ä¸€èˆ¬ä½¿ç”¨ LSTM å’Œ GRU è€Œé SRNã€‚

ç¤ºä¾‹ï¼š

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 32))
model.add(keras.layers.SimpleRNN(32))
# è¾“å…¥timestepsx32çš„äºŒé˜¶å¼ é‡,è¾“å‡º32ç»´å‘é‡,å³åºåˆ—åˆ°ç±»åˆ«æ¨¡å¼
model.summary()
# Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, None, 32)          320000    
# _________________________________________________________________
# simple_rnn (SimpleRNN)       (None, 32)                2080      
# =================================================================
# Total params: 322,080
# Trainable params: 322,080
# Non-trainable params: 0
# _________________________________________________________________
```

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 32))
model.add(keras.layers.SimpleRNN(32, return_sequences=True))
# è¾“å…¥timestepsx32çš„äºŒé˜¶å¼ é‡,è¾“å‡ºtimestepsx32çš„äºŒé˜¶å¼ é‡,å³åºåˆ—åˆ°åºåˆ—æ¨¡å¼
model.summary()
# Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, None, 32)          320000    
# _________________________________________________________________
# simple_rnn (SimpleRNN)       (None, None, 32)          2080      
# =================================================================
# Total params: 322,080
# Trainable params: 322,080
# Non-trainable params: 0
# _________________________________________________________________
```

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 32))
model.add(keras.layers.SimpleRNN(32, return_sequences=True))
# SRNçš„å †å 
model.add(keras.layers.SimpleRNN(32, return_sequences=True))
model.add(keras.layers.SimpleRNN(32, return_sequences=True))
model.add(keras.layers.SimpleRNN(32))
model.summary()
# Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, None, 32)          320000    
# _________________________________________________________________
# simple_rnn (SimpleRNN)       (None, None, 32)          2080      
# _________________________________________________________________
# simple_rnn_1 (SimpleRNN)     (None, None, 32)          2080      
# _________________________________________________________________
# simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      
# _________________________________________________________________
# simple_rnn_3 (SimpleRNN)     (None, 32)                2080      
# =================================================================
# Total params: 328,320
# Trainable params: 328,320
# Non-trainable params: 0
# _________________________________________________________________
```





### LSTM

LSTM å±‚ã€‚

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `units`ï¼šè¾“å‡ºç©ºé—´çš„è§„æ¨¡



ç¤ºä¾‹ï¼š

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(10000, 16))
# å°†è§„æ¨¡ä¸º10000çš„è¯å…¸åµŒå…¥åˆ°16ç»´å‘é‡
# è¾“å…¥256ç»´å‘é‡,è¾“å‡º256x16çš„äºŒé˜¶å¼ é‡
model.add(keras.layers.LSTM(64))
# è¾“å…¥256x16çš„äºŒé˜¶å¼ é‡,è¾“å‡º64ç»´å‘é‡(éšçŠ¶æ€),å³åºåˆ—åˆ°ç±»åˆ«æ¨¡å¼
model.add(keras.layers.Dense(16, activation='relu'))
# å…¨è¿æ¥å±‚,ReLUæ¿€æ´»å‡½æ•°,åˆ†ç±»å™¨
model.add(keras.layers.Dense(1, activation='sigmoid'))  
# å…¨è¿æ¥å±‚,Logisticæ¿€æ´»å‡½æ•°
model.summary()
# Model: "sequential"
# _________________________________________________________________
# Layer (type)                 Output Shape              Param #   
# =================================================================
# embedding (Embedding)        (None, None, 16)          160000    
# _________________________________________________________________
# lstm (LSTM)                  (None, 64)                20736     
# _________________________________________________________________
# dense (Dense)                (None, 16)                1040      
# _________________________________________________________________
# dense_1 (Dense)              (None, 1)                 17        
# =================================================================
# Total params: 181,793
# Trainable params: 181,793
# Non-trainable params: 0
# _________________________________________________________________
```

```python
model = keras.Sequential()
model.add(keras.layers.LSTM(64, 
                            dropout=0.2,
                            recurrent_dropout=0.2,
                            return_sequences=True,
                            # å †å äº†rnnå±‚,å¿…é¡»åœ¨æ¯ä¸ªæ—¶é—´æ­¥é•¿è¾“å‡º
                            input_shape=(None, df.shape[-1])))
                            # å°½ç®¡è¿™é‡Œåºåˆ—çš„é•¿åº¦æ˜¯ç¡®å®šçš„(120),ä½†ä¹Ÿä¸å¿…ä¼ å…¥
model.add(keras.layers.LSTM(64,
                            activation='relu',
                            dropout=0.2,
                            recurrent_dropout=0.2))
# äº¦å¯ä»¥ä½¿ç”¨ layers.GRU
model.add(keras.layers.Dense(1))
```



### GRU

GRU å±‚ã€‚



### Bidirectional

åŒå‘ RNN å±‚åœ¨æŸäº›ç‰¹å®šçš„ä»»åŠ¡ä¸Šæ¯”ä¸€èˆ¬çš„ RNN å±‚è¡¨ç°å¾—æ›´å¥½ï¼Œç»å¸¸åº”ç”¨äº NLPã€‚

RNN çš„è¾“å…¥åºåˆ—å­˜åœ¨é¡ºåºï¼Œæ‰“ä¹±æˆ–ååºéƒ½ä¼šå½»åº•æ”¹å˜ RNN ä»åºåˆ—ä¸­æå–çš„ç‰¹å¾ã€‚åŒå‘ RNN åŒ…å«ä¸¤ä¸ªä¸€èˆ¬çš„ RNNï¼Œåˆ†åˆ«ä»ä¸€ä¸ªæ–¹å‘å¤„ç†è¾“å…¥åºåˆ—ã€‚åœ¨è®¸å¤š NLP ä»»åŠ¡ä¸­ï¼Œåå‘å¤„ç†è¾“å…¥åºåˆ—èƒ½å¤Ÿè¾¾åˆ°ä¸æ­£å‘å¤„ç†ç›¸å½“çš„ç»“æœï¼Œå¹¶ä¸”æå–å‡ºä¸åŒä½†åŒæ ·æœ‰æ•ˆçš„ç‰¹å¾ï¼Œæ­¤ç§æƒ…å†µä¸‹åŒå‘ RNN å°†æ•è·åˆ°æ›´å¤šçš„æœ‰ç”¨çš„æ¨¡å¼ï¼Œä½†ä¹Ÿä¼šæ›´å¿«åœ°è¿‡æ‹Ÿåˆã€‚

![Screenshot from 2020-09-28 19-00-29.png](https://i.loli.net/2020/09/28/i2V36gZhtIv7BJA.png)

`keras`ä¸­ï¼Œ`Bidirectional`å®ç°ä¸ºåˆ›å»ºä¸€ä¸ªå‚æ•°æŒ‡å®šçš„ RNN å±‚ï¼Œå†åˆ›å»ºä¸€ä¸ªç›¸åŒçš„ RNN å±‚å¤„ç†ååºçš„è¾“å…¥åºåˆ—ã€‚

ç¤ºä¾‹ï¼š

```python
model = keras.Sequential()
model.add(keras.layers.Embedding(max_features, 32))
model.add(keras.layers.Bidirectional(keras.layers.LSTM(32)))
model.add(keras.layers.Dense(1, activation='sigmoid'))
```





### Dropout

ç¤ºä¾‹ï¼š

```python
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))
```





## model

![](https://i.loli.net/2020/09/27/hvxUc9eyiqJkGVu.png)

### compile

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `optimizer`ï¼šä¼˜åŒ–æ–¹æ³•ï¼Œå¯ä»¥é€‰æ‹©`'rmsprop','adam'`ç­‰ï¼Œé»˜è®¤ä¸º`'rmsprop'`
+ `loss`ï¼šæŸå¤±å‡½æ•°ï¼Œå¯ä»¥é€‰æ‹©`'binary_crossentropy','categorical_crossentropy','sparse_categorical_crossentropy','mse','mae'`ç­‰
+ `metrics`ï¼šè¯„ä»·æŒ‡æ ‡ï¼Œå¯ä»¥é€‰æ‹©`'accuracy','mse','mae','precision','recall','auc'`ç­‰ä¸­çš„å¤šä¸ª



### fit

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `x`ï¼šè¾“å…¥æ•°æ®
+ `y`ï¼šè¾“å‡ºæ ‡ç­¾
+ `batch_size`ï¼šæ¯ä¸ª`batch`çš„è§„æ¨¡
+ `epoch`ï¼šè®­ç»ƒé›†çš„å¾ªç¯è¿­ä»£æ¬¡æ•°
+ `verbose`ï¼šæ§åˆ¶å°è¾“å‡ºä¿¡æ¯ï¼Œ0=æ— è¾“å‡ºï¼Œ1=è¾“å‡ºè¿›åº¦æ¡ï¼Œ2=å¯¹æ¯ä¸ªepochè¾“å‡ºä¸€è¡Œè®°å½•ï¼Œé»˜è®¤ä¸º1
+ `callbacks`ï¼šå›è°ƒ
+ `validation_split`ï¼šæŒ‡å®šä»è®­ç»ƒé›†åˆ’åˆ†éªŒè¯é›†çš„æ¯”ä¾‹
+ `validation_data`ï¼šæŒ‡å®šéªŒè¯é›†



### evaluate

å…¶åŒ…å«çš„ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

+ `x`ï¼šè¾“å…¥æ•°æ®
+ `y`ï¼šè¾“å‡ºæ ‡ç­¾
+ `verbose`ï¼šæ§åˆ¶å°è¾“å‡ºä¿¡æ¯ï¼Œ0=æ— è¾“å‡ºï¼Œ1=è¾“å‡ºè¿›åº¦æ¡ï¼Œé»˜è®¤ä¸º1



### summary

`summary()`æ–¹æ³•æ˜¾ç¤ºæ¨¡å‹çš„åŸºæœ¬ç»“æ„ã€‚



### Sequential

`Sequential`è¿”å›ä¸€ä¸ª`keras.Model`å¯¹è±¡ã€‚`Sequential`æ¨¡å‹å°†å„å±‚çº¿æ€§ç»„åˆï¼Œé€‚ç”¨äºFNNï¼ŒCNNï¼ŒRNNï¼Œå…¶ä¸­æ¯ä¸€å±‚éƒ½æœ‰**ä¸€ä¸ªè¾“å…¥å¼ é‡å’Œä¸€ä¸ªè¾“å‡ºå¼ é‡** ã€‚

ä»¥ä¸‹`Sequential`æ¨¡å‹ï¼Œ

```python
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu", name="layer1"),
        layers.Dense(3, activation="relu", name="layer2"),
        layers.Dense(4, name="layer3"),
    ]
)
# Call model on a test input
x = tf.ones((3, 3))
y = model(x)
```

ç­‰æ•ˆäºä»¥ä¸‹åŠŸèƒ½

```python
# Create 3 layers
layer1 = layers.Dense(2, activation="relu", name="layer1")
layer2 = layers.Dense(3, activation="relu", name="layer2")
layer3 = layers.Dense(4, name="layer3")

# Call layers on a test input
x = tf.ones((3, 3))
y = layer3(layer2(layer1(x)))
```

`Sequential`æ¨¡å‹ä¹Ÿå¯ä»¥ç”¨`add()`æ–¹æ³•åˆ›å»º

```python
model = keras.Sequential()
model.add(layers.Dense(2, activation="relu"))
model.add(layers.Dense(3, activation="relu"))
model.add(layers.Dense(4))
```



CNNæ¨¡å‹ç¤ºä¾‹ï¼š

```python
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
# FNN, CNN éœ€è¦æŒ‡å®šè¾“å…¥å¼ é‡çš„shape
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))

model.add(layers.Dense(10))
```



### è‡ªå®šä¹‰æ¨¡å‹

Keras æ¨¡å‹ä»¥ç±»çš„å½¢å¼å‘ˆç°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿ `tf.keras.Model` è¿™ä¸ª Python ç±»æ¥å®šä¹‰è‡ªå·±çš„æ¨¡å‹ã€‚åœ¨ç»§æ‰¿ç±»ä¸­ï¼Œæˆ‘ä»¬éœ€è¦é‡å†™ `__init__()` ï¼ˆæ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–ï¼‰å’Œ `call(input)` ï¼ˆæ¨¡å‹è°ƒç”¨ï¼‰ä¸¤ä¸ªæ–¹æ³•ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦å¢åŠ è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚

```python
class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()     # Python 2 ä¸‹ä½¿ç”¨ super(MyModel, self).__init__()
        # æ­¤å¤„æ·»åŠ åˆå§‹åŒ–ä»£ç ï¼ˆåŒ…å« call æ–¹æ³•ä¸­ä¼šç”¨åˆ°çš„å±‚ï¼‰ï¼Œä¾‹å¦‚
        # layer1 = tf.keras.layers.BuiltInLayer(...)
        # layer2 = MyCustomLayer(...)

    def call(self, input):
        # æ­¤å¤„æ·»åŠ æ¨¡å‹è°ƒç”¨çš„ä»£ç ï¼ˆå¤„ç†è¾“å…¥å¹¶è¿”å›è¾“å‡ºï¼‰ï¼Œä¾‹å¦‚
        # x = layer1(input)
        # output = layer2(x)
        return output

    # è¿˜å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ–¹æ³•
```

ä»¥ä¸‹ä¸ºè‡ªå®šä¹‰çº¿æ€§æ¨¡å‹ç¤ºä¾‹ï¼š

```python
class Linear(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.dense = tf.keras.layers.Dense(
            units=1,
            activation=None,
            kernel_initializer=tf.zeros_initializer(),
            bias_initializer=tf.zeros_initializer()
        )

    def call(self, input):
        output = self.dense(input)
        return output
```



## regularizers

```python
model = models.Sequential()
model.add(layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),
                                              # l2 regularization, coefficient = 0.001
activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, kernel_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.001),                                    # l1 & l2
activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
```





