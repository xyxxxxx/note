## 训练集，验证集与测试集

确保训练集，验证集与测试集的划分是随机的：可以采用打乱数据集或随机抽样的任意一种方法实现。否则可能导致训练集，验证集和测试集的数据不对应于同一个问题。





## 过拟合与欠拟合

过拟合与欠拟合发生在几乎每个机器学习问题中，其来源于优化与泛化之间的矛盾。优化指训练模型以在训练集上有最好的表现（即拟合最好），而泛化指模型在从未见过的数据上的表现。我们的目标是好的泛化，但只能通过控制优化来间接控制泛化。

训练开始时，优化和泛化的方向是相同的：训练集的损失降低，验证集的损失也降低，此时称模型欠拟合；经过一些训练后，优化和泛化分道扬镳：训练集的损失降低，验证集的损失反而升高，此时称模型过拟合。

神经网络模型比较容易就能较好地拟合训练集，但是记住：真正的挑战是泛化，而非优化。

为了防止模型过拟合，最佳方案是**获取尽可能多的数据**，这会使模型自然地泛化更好。其次是限制模型的容量（参数数量），使其仅记忆一些最重要的模式。

### 降低网络规模

模型的容量不应过大，也不应过小。若过大，则模型将非常快地进入过拟合，且在整个过程中表现不稳定；若过小，则模型将非常慢地训练，甚至没有足够的拟合能力。

目前仅能凭经验确定层数和层规模。找到合适的模型的通常流程是从很少的层数和规模开始，逐渐增加层数和规模直到过拟合成为限制模型表现的主要原因。

### 正则化

正则化可以有效地限制参数的取值。

正则化会降低模型的拟合能力，意味着也降低模型的过拟合能力。

### 遗忘

遗忘会降低训练速度，但能够提高模型的拟合能力，同时降低模型的过拟合能力。



## 调参

这将是最耗费时间的一步：调整模型，训练，测试，再调整模型……直到模型的表现尽可能好。在这个过程中你应该尝试：

+ 增加遗忘
+ 增加或减少层数和层规模
+ 增加 L1 或 L2 正则化
+ 尝试不同的超参数
+ 回到特征工程

需要注意，如果多次使用同一个验证集，那么将导致模型也会对于这个验证集过拟合。



## 梯度爆炸与梯度消失

无论是梯度消失还是梯度爆炸，从本质上来讲都是因为梯度反向传播中的连乘效应。

前馈神经网络的梯度消失会造成前面层的参数无法更新；循环神经网络的梯度消失会造成前面层的隐状态不影响参数的更新。

梯度爆炸会造成参数剧烈变化，使得学习不稳定甚至中断。







## LSTM, GRU

如果将无用的特征拼接到输入向量中，可能不仅不能提高，反而会降低模型的效果。



