# 模型压缩

## 参考

* [【機器學習2021】神經網路壓縮 (Network Compression) (一) - 類神經網路剪枝 (Pruning) 與大樂透假說 (Lottery Ticket Hypothesis)](https://www.youtube.com/watch?v=gmsMY5kc-zw&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=36)
* [【機器學習2021】神經網路壓縮 (Network Compression) (二) - 從各種不同的面向來壓縮神經網路](https://www.youtube.com/watch?v=gmsMY5kc-zw&list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&index=37)

## 剪枝（Pruning）

## 知识蒸馏（Knowledge Distillation）

### 论文

* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)

## 量化（Quantization）

### 论文

* [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/abs/1511.00363)：使用二元权重 +1 和 -1；二元权重可以起到防止过拟合的效果

## 动态计算（Dynamic Computation）
 
### 论文

* [Multi-Scale Dense Networks for Resource Efficient Image Classification](https://arxiv.org/abs/1703.09844)：使用动态深度
* [Slimmable Neural Networks](https://arxiv.org/abs/1812.08928)：使用动态宽度
* [SkipNet: Learning Dynamic Routing in Convolutional Networks](https://arxiv.org/abs/1711.09485)
