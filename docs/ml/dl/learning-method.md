[toc]



# 集成学习

给定一个学习任务，假设输入 $\pmb x$ 和输出 $\pmb y$ 的真实关系为 $\pmb y = h(\pmb x)$。对于 $M$ 不同的模型 $f_1(\pmb x), ⋯ , f_M(\pmb x)$，每个模型的期望错误为
$$
R(f_m)=E((f_m(\pmb x)-h(\pmb x))^2)=E(\epsilon_m^2(\pmb x))
$$
其中 $\epsilon_m(\pmb x)=f_m(\pmb x)-h(\pmb x)$ 为模型 $m$ 在样本 $\pmb x$ 上的错误。

那么所有模型的平均错误为
$$
\bar{R}(f)=\frac{1}{M}\sum_{m=1}^M E(\epsilon_m^2(\pmb x))
$$
**集成学习(Ensemble Learning)**就是通过某种策略将多个模型集成起来，通过群体决策来提高决策准确率。集成学习首要的问题是如何集成多个模型，比较常用的集成策略有直接平均、加权平均等。

最直接的集成学习策略就是直接平均，即“投票”。基于投票的集成模型 $F(\pmb x)$ 为
$$
F(\pmb x)=\frac{1}{M}\sum_{m=1}^M f_m(\pmb x)
$$
**定理** 对于 $M$ 个不同的模型 $f_1(\pmb x), ⋯ , f_M(\pmb x)$，其平均期望错误为 $\bar{R}(f)$，基于简单投票机制的集成模型 $F(\pmb x)=\frac{1}{M}\sum_{m=1}^M f_m(\pmb x)$，则 $F(\pmb x)$ 的期望错误在 $\frac{1}{M}\bar{R}(f)$ （如果每个模型的错误不相关）和 $\bar{R}(f)$ （如果每个模型的错误完全相同）之间。

> 证明略。

由上述定理可知，为了得到更好的集成效果，需要模型之间具有一定的差异性。并且随着模型数量的增多，期望错误也会下降，并趋近于0。

集成学习的思想可以用一句古老的谚语来描述：“三个臭皮匠赛过诸葛亮”。但是一个有效的集成需要各个基模型的差异尽可能大，为了增加模型之间的差异性，可以采取 Bagging 和 Boosting 这两类方法：

+ **Bagging 类方法**通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性，代表性方法有 Bagging 和随机森林等：
  + **Bagging(Bootstrap Aggregating)**通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。我们在原始训练集上进行有放回的随机采样，得到 $M$ 个比较小的训练集并训练 $M$ 个模型，然后通过投票的方法进行模型集成。
  + **随机森林(Random Forest)**[Breiman, 2001] 在 Bagging 的基础上再引入了随机特征，进一步提高每个基模型之间的独立性。在随机森林中，每个基模型都是一棵决策树。
+ **Boosting 类方法**按照一定的顺序来先后训练不同的基模型，每个模型都针对前序模型的错误进行专门训练，根据前序模型的结果调整训练样本的权重，从而增加不同基模型之间的差异性。Boosting类方法是一种非常强大的集成方法，只要基模型的准确率比随机猜测高，就可以通过集成方法来显著地提高集成模型的准确率。Boosting类方法的代表性方法有AdaBoost[Freund et al., 1996]等。



## 随机森林

### 决策树

决策树的思想在日常生活中俯拾皆是，来看下面的对话：

   *母亲：周末给你安排相个亲，朋友亲戚家的小伙子。*

   *女儿：那人多大？*

   *母亲：26。*

   *女儿：长的帅不帅？*

   *母亲：挺帅的。*

   *女儿：收入高不高？*

   *母亲：不算很高，中等吧。*

   *女儿：做什么职业的？*

   *母亲：公务员，在区税务局上班。*

   *女儿：那好，我去见见。*

女儿决定是否去相亲的决策过程就是典型的分类树决策。假设她对相亲对象的要求是：30岁以下、长相中等以上、高收入或者中等收入并且是公务员，那么可以用下图表示她的决策逻辑：

![](https://i.loli.net/2021/02/24/QvcMSJthAE1q7pD.png)

这幅图还不能算作严格意义上的决策树，因为长相和收入的判断标准没有量化。如果将所有条件量化，就变成真正的决策树了。

**决策树(decision tree)**是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个*特征*上的测试，每个分支代表这个特征在某个值域上的输出，而每个叶节点存放一个*类别*。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。



### ID3算法

ID3算法用**信息增益**（即信息论中的互信息 $I(X,Y)$ ）来判断当前节点应该用什么特征来构建决策树。信息增益越大，则越适合用来分类。

> 回顾互信息的性质：互信息衡量已知一个变量时，另一个变量不确定性的减少程度。独立变量之间的互信息为0，表示互相之间在减少变量的不确定性上完全没有帮助；变量与其自身的互信息为其熵本身，表示变量完全消除了其自身的不确定性。



考虑下图的例子，共有10个**样本**，其中第一至三列为**特征**，第四列为**输出类别**：

![](https://images.cnblogs.com/cnblogs_com/leoo2sk/WindowsLiveWriter/34d255f282ae_B984/2_3.png)

用L, F, A, D分别表示这4列，计算各特征的信息增益：
$$
H(D)=-(0.7\log 0.7+0.3\log 0.3)=0.8813\\
H(D|L)=-(0.1\log \frac{0.1}{0.3}+0.2\log \frac{0.2}{0.3}+0.3\log \frac{0.3}{0.4}+0.1\log \frac{0.1}{0.4}+0.3\log \frac{0.3}{0.3}+0\log \frac{0}{0.3})=0.6000\\
L(D,L)=0.8813-0.6000=0.2813 \\
H(D|F)=-(0.1\log \frac{0.1}{0.4}+0.3\log \frac{0.3}{0.4}+0.4\log \frac{0.4}{0.4}+0\log \frac{0}{0.4}+0.2\log \frac{0.2}{0.2}+0\log \frac{0}{0.2})=0.3245\\
L(D,F)=0.8813-0.3245=0.5568 \\
H(D|A)=-(0.4\log \frac{0.4}{0.5}+0.1\log \frac{0.1}{0.5}+0.3\log \frac{0.3}{0.5}+0.2\log \frac{0.2}{0.5})=0.8464\\
L(D,A)=0.8813-0.8464=0.0349 \\
$$
F具有最大的信息增益，因此使用该特征进行分类（或者说分裂该特征）：

![](https://images.cnblogs.com/cnblogs_com/leoo2sk/WindowsLiveWriter/34d255f282ae_B984/3_3.png)

对于各个分支再递归调用此方法，就可以最终得到整个决策树。



ID3算法简单直观，容易构造决策树，但是缺陷也很明显：（详见[ID3算法的缺陷](https://zhuanlan.zhihu.com/p/89901519)）

+ 不能处理连续特征

+ 优先采用信息增益大的特征建立决策树的节点，这导致其倾向于选择取值更多的特征作为节点

  > 这可以理解为一种过拟合。

+ 没有考虑缺失值的情况

+ 没有考虑过拟合的问题



### C4.5

C4.5算法在ID3算法的基础上，针对上述问题进行了改进：（详见[决策树之C4.5算法](https://zhuanlan.zhihu.com/p/89902999)）

+ ID3不能处理连续特征，C4.5的思路是将连续的特征离散化。例如m个样本有m个连续特征a，将它们从小到大排序为 $a_1,a_2,\cdots,a_m$，则C4.5取每两个相邻值的平均数，共m-1个候选划分点，其中第i个划分点Ti表示为： $T_i=(a_i+a_{i+1})/2$。对于这m-1个点，分别计算以该点作为二分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为at，则小于at的值为类别1，大于at的值为类别2，这样我们就做到了连续特征的离散化。将一个连续特征划分为多个区间只需要进行多次二分。

+ ID3倾向于选择取值更多的特征作为节点，C4.5引入一个称为信息增益率（或信息增益比）的变量 $I_R(X,Y)$，其为信息增益和特征熵（即某一特征的熵）的比值：
  $$
  I_R(D,A)=\frac{I(D,A)}{H(A)}
  $$
  其中D为输出类别，A为特征。特征数越多的特征对应的特征熵越大，将其作为分母可以校正信息增益容易偏向于取值较多的特征的问题。

  然而这样做可能导致矫枉过正，即反过来倾向于选择取值更少的特征作为节点。因此C4.5采用了一个启发式算法，先从候选特征中找出信息增益高于平均水平的特征，再从这些特征中选择信息增益率最高的特征。

+ ID3不能处理缺失值，C4.5将缺失值处理分解为两个子问题：

  + 在样本的一些特征取值缺失时，如何选择划分的特征。采取的方法是对于某一特征，使用那些有取值的样本计算信息增益，最后乘以一个系数，如下图示例：

    图

  + 选择某一特征后，如何处理取值缺失的样本。采取的方法是将取值缺失的样本的权重按比例划分到所有取值中，如下图示例：

    （在上面例子的基础上，选择“纹理”这一特征，处理“稍糊”这一取值分支，其中“纹理”取值缺失的样本8和10在这里有权重5/15）

    图

+ ID3不能防止过拟合，C4.5引入了正则化系数进行初步的剪枝。



C4.5算法仍存在改进的空间：

+ C4.5的剪枝方法有优化的空间。决策树剪枝的具体思路将在CART算法部分介绍。
+ C4.5生成的是多叉树，若采用二叉树，计算机的运算效率会更高。
+ C4.5只能用于分类，不能用于回归。
+ C4.5的熵模型包含大量的对数运算，对于连续特征还有排序操作。如果能简化这部分运算的话就更好。



### CART

ID3和C4.5算法采用信息增益（比）来选择特征，涉及大量的对数计算。







## AdaBoost算法

Boosting 类集成模型的目标是学习一个**加性模型(additive model)**
$$
F(\pmb x)=\sum_{m=1}^M\alpha_m f_m(\pmb x)
$$
其中 $f_m(\pmb x)$ 为**弱分类器(weak classifier)**或**基分类器(base classifier)**， $\alpha_m$ 为弱分类器的集成权重， $F(\pmb x)$ 称为**强分类器(strong classifier)**。

Boosting 类方法的关键是如何训练每个弱分类器 $f_m(\pmb x)$ 及其权重 $\alpha_m$。为了提高集成的效果，应当尽量使得每个弱分类器的差异尽可能大。一种有效的算法是采用迭代的方法来学习每个弱分类器，即按照一定的顺序依次训练每个弱分类器。假设已经训练了 $m$ 个弱分类器，在训练第第 $m+1$ 个弱分类器时，<u>增加已有弱分类器分错样本的权重</u>，使得第 $m+1$ 个弱分类器“更关注”于已有弱分类器分错的样本。这样增加每个弱分类器的差异，最终提升集成分类器的准确率。这种方法称为**AdaBoost(Adaptive Boosting)**算法。

AdaBoost 算法是一种迭代式的训练算法，通过改变数据分布来提高弱分类器的差异。在每一轮训练中，增加分错样本的权重，减少分对样本的权重，从而得到一个新的数据分布。

以二分类为例，弱分类器 $f_m(\pmb x)\in \{+1,-1 \}$，AdaBoost算法的训练过程如下图所示。最初赋予每个样本同样的权重，在每一轮迭代中，根据当前的样本权重训练一个新的弱分类器，然后根据这个弱分类器的错误率来计算其集成权重，并调整样本权重。

![](https://i.loli.net/2021/02/24/m9dLkwnbq3PguaM.png)





