# 嵌入

## 讨论

下面介绍的各种词嵌入方法都是将每一个 token 表示为一个固定的、预训练的向量，在大型语言模型（如 BERT、GPT 等）开始使用自注意力层产生考虑上下文的词嵌入（contextualized word embedding）之后，这些传统的方法就过时了。

## word2vec

## glove

## FastText
