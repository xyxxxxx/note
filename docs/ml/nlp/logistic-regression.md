# 逻辑回归

朴素贝叶斯和逻辑回归之间的最大区别在于前者是一个**生成式（generative）**分类器，而后者是一个**区分式（discriminative）**分类器。

建立一个机器学习模型有两种非常不同的框架。打一个视觉上的比方，假设我们要区分狗的图片和猫的图片，生成式模型的思路就是让模型理解狗的模样和猫的模样，当给出一张测试图片时，模型会检查是狗的模样还是猫的模样更匹配它，因而给出标签；区分式模型的思路则是学习狗和猫区别，至于狗和猫的模样本身则并不重要。

更正式地，回想朴素贝叶斯将文本 $d$ 划分到类别 $c$ 的条件概率 $P(c|d)$ 通过计算似然和先验得到：

$$
\hat{c}=\arg\max_{c\in C}P(d|c)P(c)
$$

生成式模型就是利用似然这一项，它表示如果已知类别是 $c$，那么如何生成文本 $d$ 的一系列特征。

与之相对地，区分式模型在文本分类场景尝试的是直接计算 $P(c|d)$。它会为那些能够帮助它区分不同类别的特征学习到高的权重，尽管它并不能为某个类别生成一个实例。

逻辑回归相比朴素贝叶斯有一些优势：朴素贝叶斯有很强的条件独立的假定，但是各条件很可能相关；逻辑回归对于相关特征的健壮性也更强，因为它可以将权重拆分到这几个特征上。因此当特征相关时，逻辑回归预测的概率会比朴素贝叶斯更准确；因此在大型的数据集上普遍表现更好，是一个默认的选项。

尽管如此，朴素贝叶斯方法在小型数据集和短文本上依然表现很好，并且易于实现、训练迅速，因此在某些情况下也是合理的选择。

在许多情况下，我们不仅想要正确的分类，还想知道分类器为何做出了这样的选择，也就是让模型**可解释（interpretable）**。例如在逻辑回归模型中，一种理解分类器的决策的方法是理解每一个特征做出了多大的贡献。检查某个特定的特征是否重要可以通过统计检验方法，或者查看它的权重值，这能够帮助我们解释分类器的决策。
