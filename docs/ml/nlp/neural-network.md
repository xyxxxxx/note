# 神经网络

## 神经语言模型

作为前馈神经网络的第二个应用，让我们考虑语言建模：根据之前的词上下文预测下一个词。神经语言建模本身是一个重要的 NLP 任务，并且也在机器翻译、总结、语音识别、语法纠正和对话等任务的诸多重要算法中扮演角色。我们这里描述一个简单的前馈神经语言模型，最初由 Bengio 等人（2003）引入。尽管现代的神经语言模型使用更强大的架构，例如循环网络或 transformer 网络，前馈网络引入了神经语言建模的许多重要概念。

神经语言模型相比 n 元序列模型有诸多优势。相比 n 元序列模型，神经语言模型可以处理更长的历史，可以在相似词的上下文中泛化得更好，并且在词预测上更加精确。另一方面，神经语言模型要复杂得多，训练花费的时间更长，解释性也不如 n 元序列模型，因此对于许多小型任务，n 元序列模型仍然是正确的工具。

前馈神经语言建模就是一个前馈神经网络，其在时间 $t$ 接受一些之前的词的表示作为输入、输出可能的下一个词的概率分布。就像 n 元序列语言建模一样，前馈神经语言建模将给定完整历史的词概率近似为基于前 N-1 个词：

$$
P(w_t|w_1^{t-1})\approx P(w_t|w_{t-N+1}^{t-1})
$$

在下面的例子中我们将使用四元序列，因此我们将展示一个估计概率 $P(w_t=i|w_{t-3},w_{t-2},w_{t-1})$ 的网络。

神经语言模型将上下文中的词用它们的嵌入来表示，而不是像 n 元序列语言模型一样直接用这些词本身来表示。使用嵌入使得神经语言模型能够对未见过的数据泛化得更好。例如，假设我们在训练中见过下面这个句子：

<pre>
    I have to make sure that the cat gets fed.
</pre>

但从没有见过 `dog gets fed`。我们的测试集中有前缀 `I forgot to make sure that the dog gets`，那么下一个词是什么？一个 n 元序列语言模型会在 `the cat gets` 之后预测 `fed`，但不会在 `the dog gets` 之后预测 `fed`。但是一个神经语言建模，会知道 `cat` 和 `dog` 具有相似的嵌入，从而能够从 `cat` 上下文泛化到为 `dog` 之后的 `fed` 赋予一个足够高的概率。
