[toc]

# 词语义

我们应当如何表示词的含义？在之前的n-gram模型和传统NLP应用中，词的表示就是词的字符串或者词汇表的索引号。这种表示和哲学领域的一个习惯十分相似，即用词的small capital表示词的含义，例如猫的含义表示为ᴄᴀᴛ，狗的含义表示为ᴅᴏɢ。

但是这样的表示并不能让人满意，你可能看过下面这个古老的哲学笑话：

​        Q: What’s the meaning of life?
​        A: ʟɪꜰᴇ

我们当然能做得更好！我们想要的词义表示模型应当能够告诉我们，一些词具有相似的含义（例如“猫”和“狗”）而一些词具有相反的含义（例如“冷”和“热”），一些词具有褒义（例如“鼓励”）而一些词具有贬义（例如“怂恿”），一些词是从不同角度对同一个事件的描述（例如“买”、“卖”和“付款”）。

更普遍地，词义模型应能够做出有用的推断，以帮助我们解决词义相关的任务如问答、总结、检测短语或俗语、对话等。

**词元和词义**

我们从词典中某个词的定义开始：

mouse (N)
1. any of numerous small rodents...
2. a hand-operated device that controls a cursor...

这里的mouse就是**词元(lemma)**，或者称为**词典形(citation form)**。mouse是mice的词元，并且词典上也仅有mouse这一词条。类似地，sing是sing, sang, sung的词元。在许多语言如西文中，不定式作为动词的词元，例如dormir是duermes的词元。mice, sang, sung则称为（词元的不同）**词形(word form)**。

词元mouse显示同一词元可以有多个含义，我们将每一种含义称为一个**词义(word sense)**。词元的多义性可能使解释变得困难，**词义消歧(word sense disambiguation)**任务就是确定在特定上下文中词使用何种词义。

> 这里将word sense翻译为“词义”，word meaning也翻译为“词的含义”或“词义”（在尽量避免歧义的条件下）。

**同义词/近义词**

关于词义很重要的一点是不同词义之间的关系。例如当两个词具有完全相同或几乎相同的词义时，我们称之为**同义词(或近义词, synonymy)**，例如下面的词对：

​        couch/sofa    vomit/throw up    car/automobile

同义词的一个更加正式的定义是两个词是同义词，当它们可以任何句子中互相替换而不改变句子的真值条件。在这种情况下，我们经常称这两个词有相同的**命题意义(propositional meaning)**。

但就算同义词的替换不改变句子的真值，同义词在词义上也不是完全相同——事实上，几乎没有两个词的词义是完全相同的。语义学的基本原理之一，**对比原理(principle of contrast)**假定语言形式上的差异必定（或多或少地）导致含义上的差异。例如，H2O常在化学、生物等学科的语境中使用，而不适合出现在大多数日常情景中。这种体裁（风格/使用场景）上的区别也是词义的一部分。因此在实践中，同义词(synonymy)更接近于描述词义相近的关系（即近义词）。

**词相似度**

尽管很少有词有很多同义词，但大多数词都有很多**相似(similar)**词。“猫”和“狗”不是同义词，但在一定程度上是相似词。从同义到相似，我们从处理词义的关系转换到处理词的关系（即相似度）。处理词避免了对词义采用特别的表示，因而简化了我们的任务。

**相似度(similarity)**这个概念在大型的语义任务中十分有用。已知两个词的相似程度有助于计算两个短语或句子的相似程度，而这些是自然语言理解中的重要任务。一种获得词相似度的定量结果的方法是请人进行打分，一些数据集就是从这样的实验中得到，例如SimLex-999数据集(Hill et al., 2015)给出了从0到10的相似度度量，下面是一些例子：

![](https://i.loli.net/2021/01/05/Af4EaWxjuiTwbGZ.png)

**词相关性**

两个词的词义除了相似之外还会在其它方面相关，这当中的一种关系就是词**相关性(relatedness)**(Budanitsky
and Hirst, 2006)，在心理学领域习惯上称为**关联(association)**。

考虑词“茶杯”和“咖啡”的含义，它们并不相似（即没有共同的特征），但它们明显相关（即共同参与到喝咖啡这样一个日常事件中）。类似地，“外科医生”和“手术刀”不相似，但相关。

一种常见的词之间的相关性是它们同属一个**语义场(semantic field)**。所谓语义场就是涉及一个特定语义情景并且有结构性关联的一系列词，例如医院的语义场（“外科医生”，“手术刀”，“护士”，“麻醉”，“医院”），餐厅的语义场（“服务员”，“菜单”，“盘子”，“食物”，“厨师”），或房屋的语义场（“门”，“屋顶”，“厨房”，“家庭”，“床”）。

语义场也和**话题模型(topic model)**有关，话题模型是在大型文本集上进行无监督学习以得到若干个相关词的集合。语义场和话题模型都是对于发现文本的话题结构非常有用的工具。

**语义框架和语义角色**

**语义框架(semantic frame)**的概念和语义场关系紧密。所谓语义框架就是对一类特定事件代表不同角度或不同参与者的一系列词。例如，商业交易就是一类事件，其中一个实体用金钱向另一实体交易产品或服务，在这之后产品易手或服务实行。这一事件可以用动词编码为买（从甲方的角度），卖（从乙方的角度），付款（着眼于钱的交付）等。框架内包括**语义角色(semantic role)**（例如甲方、乙方、货物、钱），而词可以在句子中扮演这些角色。

搭建了上述语义框架后，系统就能够知道“甲从乙买书”可以转述为“乙卖书给甲”，并且甲扮演了买方而乙扮演了卖方。这种释义的能力在问答系统中非常重要，并且也能帮助机器翻译进行角度转换。

**褒贬**

最后，词包含情感含义或**褒贬(或隐含义, connotation)**。例如一些词含有**褒义(positive connotation)**（例如“不错”，“喜欢”）而另一些词含有**贬义(negative connotation)**（例如“糟糕”，“讨厌”）。语言中包含的积极或消极评价称为**情感(sentiment)**，而词的情感在情感分析、立场识别，以及NLP在政治语言、顾客评价的应用等任务上都扮演了非常重要的角色。

在情感含义上的早期工作(Osgood et al., 1957)发现词在情感含义上有三个维度的变化，通常称为

+ valence: 刺激的愉悦程度
+ arousal: 刺激唤起的情感强度
+ dominance: 刺激执行的控制程度

因此“高兴”和“满意”的valence高，而“难过”和“恼怒”的valence低；“激动”和“疯狂”的arousal高，而"放松"和“平静”的arousal低；“重要”和“控制”的dominance高，而“敬畏”和“影响”的dominance低。每个词被表示为3个数字，例如：

![](https://i.loli.net/2021/01/06/Q4WXN8tb6DGrTeJ.png)

Osgood et al. (1957)注意到使用3个数字表示一个词，则模型将所有词表示为三维空间中的一个点，词与三维空间中的向量一一对应。这种革命性的想法就是后面将要介绍的向量语义模型的雏形。





# 向量语义

那么我们如何建立一个计算模型，使得它可以处理词的各种词义？找到一个完美的，可以全方位地处理词的所有词义的模型是困难的，目前的最佳模型依然是向量语义模型，其灵感可以追溯到1950年代的语言学和哲学工作。

当时，哲学家Ludwig Wittgenstein因为质疑为所有词的含义建立一个完全的形式理论的可能性，而提出“词的含义就是词在语言中的用法”(Wittgenstein, 1953, PI 43)。也就是说，除了从逻辑上定义每个词，还可以根据人们对词的使用和理解进行定义。

看一个说明上述方法的例子。假设你不知道粤语词ongchoy的含义，但是你看到它出现在一些句子和上下文中：

> 这是一个外来语的例子。在英文语境中，ongchoy或Ong Choy是一个外来语，其来自于粤语的“蕹菜”，中文也称为“空心菜”。

+ ongchoy和蒜一起炒很好吃
+ ongchoy和米饭十分搭配
+ ……ongchoy叶子和咸酱……

再假设你看过一些其它的词出现在相似的上下文中：

+ ……蒜炒菠菜盖饭……
+ ……莙荙菜(chard)的根和叶子很好吃……
+ ……羽衣甘蓝(collard greens)和其它咸的叶子菜……

ongchoy与“米饭”、“蒜”、“好吃”和“咸”出现在一起，就如同“菠菜”、“莙荙菜”和“羽衣甘蓝”一样，因此读者可以推测ongchoy也是一种类似的叶子菜。

我们在计算中可以采用同样的方法：计数出现在ongchoy的上下文的词，我们会发现“炒”、“吃”、“蒜”之类的词，而类似的词也出现在“菠菜”或“羽衣甘蓝”的上下文中，这一现象可以帮助发现ongchoy和这些词的相似性。

因此，向量语义结合了两种直觉：用法直觉和向量直觉(Osgood et al., 1957)，将词 $w$ 的含义定义为高维语义空间的一个向量。向量语义有非常多的版本，每一种对于向量的值的确定方法都略有不同，但都与上下文的词的计数有关。

![](https://i.loli.net/2021/01/06/82aBNRUkMCHq73z.png)

用向量表示词通常称为**嵌入(embedding)**，因为词嵌入在一个特殊的向量空间中。上图展示了情感分析任务中学习到的嵌入的可视化，即将一些选定的词从原有的60维空间投影到2维空间的位置。

可以看到褒义和贬义词（也和中立词）落在了空间的不同部分。这显示了向量语义的一个重要的优点：提供了一个细粒度词义模型可以实现词相似度。在基于朴素贝叶斯方法的情感分类器中，如果测试集文本中的情感词未曾出现在训练集中，则无法提供任何信息；但如果对词进行嵌入，则可以……向量语义模型也十分切实可行，因为可以自动学习而无需复杂的人工贴标签和监督。

由于上述优点，向量语义模型已经成为NLP表示词义的标准方法。下面我们将介绍2种最常用的模型：其一是**tf-idf**模型，通常用作基线，其得到的向量长而稀疏；其二是**word2vec**模型，其发展而来的一族模型可以构建更短更稠密的具有良好语义性质的向量。





# 词和向量

## 向量和文本

我们通常使用**共现矩阵(co-occurence matrix)**来表现词共同出现(co-occur)的频率，例如**词-文本矩阵(term-document matrix)**的每一行代表一个词而每一列代表一个文本。下图展示了4个选定的词在莎士比亚的4场戏剧中的出现次数：

![](https://i.loli.net/2021/01/06/CulYMhXTjdEINez.png)

这里每个文本对应一个列向量，每个维度代表一个词。最初词-文本矩阵被定义为**信息检索(Information Retrieval, IR)**任务中寻找相似文本的方法，因为相似文本有相似的词，因而有相似的向量。

实际上，信息检索任务是从一个文本集中找到查询 $\pmb q$ 的最佳匹配文本 $d$。因为查询 $\pmb q$ 也表示为一个 $|V|$ 维向量，因此，我们需要计算向量相似度的方法。后面我们将会介绍tf-idf词权重和余弦相似度。



## 词作为向量

类似地，图6.2中的每个词对应一个行向量，每个维度代表一个文本。相似的词出现在相似的文本中，因而有相似的向量。

但是更常用的方法是使用**词-词矩阵(term-term matrix, word-word matrix)**，这是一个 $|V|\times |V|$ 矩阵，每个元素记录该列的词出现在该行的词的上下文的频率。上下文可以是整个文本，那么元素值代表两个词出现在同一个文本的次数（同一文本多次出现应求乘积）；但更常见的方法是使用目标词附近的一个窗口，例如从目标词往左数4个词，往右数4个词，称为一个±4词窗口，下面展示了一些例子：

​                       is traditionally followed by **cherry**            pie, a traditional dessert
​                                 often mixed, such as **strawberry**   rhubarb pie. Apple pie
​       computer peripherals and personal **digital**            assistants. These devices usually
​                         a computer. This includes **information** available on the internet

如果我们对于每一个词的每一次出现都对其上下文的词进行计数，就能得到一个词-词矩阵。下图展示了一个简化的词-词矩阵的一部分，计算自维基百科语料库(Davies, 2015)。可以看到cherry和strawberry比较相似（因为pie和sugar频繁出现在上下文中），而digital和information比较相似。

![](https://i.loli.net/2021/01/06/FBUsyj5Omh9efQ6.png)

上图绘制了digital和information的两个分量，意在说明此种表示下余弦相似度的合理性。

注意词-词矩阵的 $|V|$ 通常在10000~50000之间（选择词频最高的词；保留词频在第50000之后的词一般没有帮助）。词-词矩阵也是一个稀疏矩阵，可以使用对于稀疏矩阵更有效率的存储和计算方法。





# 余弦相似度

计算两个词的相似度也就是对它们的向量进行运算。NLP中最常用的相似度指标是向量夹角的余弦值
$$
\cos(\pmb v,\pmb w)=\frac{\pmb v\pmb w}{|\pmb v||\pmb w|}=\frac{\sum v_iw_i}{\sqrt{\sum v_i^2}\sqrt{\sum w_i^2}}
$$
再来对这个部分词-词矩阵计算词相似度

![](https://i.loli.net/2021/01/06/A53sPiJoyKTcxtO.png)

注意在这种表示下所有的分量都是非负整数，因此余弦相似度的取值范围为[0,1]区间。





# TF-IDF

如图6.5所示的词-词矩阵包含了词共同出现的原始频率，但这样一个频率并不是词之间关系的最佳描述，一个问题就是原始频率的区分度(discrimination)不高。例如当我们考虑何种上下文为cherry和strawberry共有而不为digital和information所有时，我们当然不会认为the, it或they这样的词具有好的区分度，因为它们会出现在所有词的上下文中。

换言之，在目标词的上下文中频繁出现的词是重要的，但在所有词的上下文中都频繁出现的词又是不重要的，因此我们就要解决这个矛盾。

tf-idf算法计算两项的积，这两项分别代表上述的两种直觉。

第一项称为**词频(term frequency)**(Luhn, 1957)，即词 $t$ 在文本 $d$ 中的频率，我们可以使用原始计数
$$
{\rm tf}_{t,d}={\rm count}(t,d)
$$
但实践中通常采用对数对取值进行压缩
$$
{\rm tf}_{t,d}=\lg({\rm count}(t,d)+1)
$$
第二项为那些只在部分特定的文本中出现的词给予更高的权重，因为这些词在区分这些特定文本和其它文本时十分有用；相对而言，那些在整个集合中频繁出现的词就作用不大。**文本频率(document frequency)**指词 $t$ 出现在多少个文本中，**集合频率(collection frequency)**则指词 $t$ 在整个集合中的出现次数。例如莎士比亚的37部戏剧中，Romeo和action有相同的集合频率，但有完全不同的文本频率：

![](https://i.loli.net/2021/01/06/X7tKyzewFVpuJWQ.png)

因此当我们需要寻找关于罗密欧的浪漫苦恼的文本时，词Romeo就应该有相当大的权重。我们通过**idf(inverse document frequency)**词权重来强调这些有区分度的词，定义为 $N/{\rm df}_t$，其中 $N$ 是集合中的文本总数，而 ${\rm df}_t$ 是词 $t$ 的文本频率。 ${\rm df}_t$ 越高，则权重越低；当 ${\rm df}_t$ 达到 $N$ 时，权重最低取1。

由于许多集合的文本数量非常多，我们同样对idf值取对数，即
$$
{\rm idf}_t=\lg (\frac{N}{{\rm df}_t})
$$
下面是莎士比亚语料库中一些词的df和idf值，可以看到df=1的Romeo的idf取到最大值，而df=N的good和sweet的idf值则直接取0

![](https://i.loli.net/2021/01/06/XuKi8R12IOfQ76T.png)

tf-idf值则是上面两项的乘积，即
$$
w_{t,d}={\rm tf}_{t,d}\times {\rm idf}_t
$$
为图6.2中的词频应用tf-idf加权后的结果如下图，可以看到good行的tf-idf=0，而fool行的tf-idf值也非常小。

![](https://i.loli.net/2021/01/06/GawyHiDJtqYrmR9.png)

tf-idf加权是为信息检索的共现矩阵加权的一种方法，但也应用在NLP的其它分支中。它也是一个好的基线，是首次尝试的选择。





# tf-idf模型的应用

总而言之，我们目前的向量语义模型是将目标词表示为词汇表规模维度的稀疏向量，其中每一维的值是这一维对应的词在目标词的上下文中出现的频率，并且用tf-idf加权；模型用余弦相似度计算两个词的相似度。这整个模型有时简称为tf-idf模型。

> ？实际上应用tf-idf加权的都是词-文本矩阵。虽然也可以照搬到词-词矩阵，但没见过这种应用……

tf-idf模型的一种用途是计算词相似度，例如我们可以通过计算任何目标词 $w$ 与其它 $V-1$ 的词的余弦相似度，来计算与 $w$ 最相似的k个词。

tf-idf模型也能用于判断两个文本是否相似。我们可以将文本表示为文本中所有词的向量的平均值，或者说**中心(centroid)**，然后就可以计算余弦相似度。

文本相似度也用于诸多任务中，例如信息检索、查重、新闻推荐系统等。





# word2vec

本节我们将介绍另一种词表示法，使用更短（长度大致在50～1000范围）且更稠密的向量。

实践证明，在任何NLP任务中稠密向量都比稀疏向量的表现更好。尽管我们还没有完全明白其原因，但有以下直觉：首先，短而稠密的向量更适合作为机器学习的特征，例如模型处理50000维的稀疏向量需要大量的参数，而大部分参数都难以得到较好的学习；其次，由于使用了更少的参数，稠密向量的泛化效果更好，有利于防止过拟合；最后，稠密向量识别同义词的效果优于稀疏向量。

我们将介绍**SGNS(skip-gram with negative sampling)**算法，它是word2vec工具包使用的两种算法之一，因此也常被称为word2vec方法(Mikolov et al. 2013, Mikolov et al. 2013a)。word2vec方法快速、高效，并且容易获取代码和预训练模型。

word2vec的想法是不去计数每一个词 $w$ 出现在目标词附近的频率，而是训练一个分类器完成二元预测任务： $w$ 是否可能出现在目标词附近？我们不是真的要去完成这个预测任务，而是将分类器学习到的权重作为词嵌入。任何文本都可以直接作为分类器的训练数据，而实际出现在目标词附近的词就会有较高的权重。

word2vec模型相比神经网络模型简单很多，这体现在两方面：第一，word2vec简化了任务，将词预测任务简化为二元分类任务；第二，word2vec简化了架构，将多层神经网络模型简化为逻辑回归模型。skip-gram的想法是：

1. 将目标词和其邻近词组合作为若干正例
2. 从词汇表随机抽取其它词与目标词组合作为若干反例
3. 使用逻辑回归训练区分上述两类的分类器
4. 使用回归权重作为嵌入



## 分类器

考虑下面的句子，其中目标词为apricot，窗口为±2大小：

![](https://i.loli.net/2021/01/07/QDA5nCiR4Sbx3y7.png)

我们的目标是训练一个分类器，使其接受目标词 $\pmb t$ 和候选词 $\pmb c$ 组成的一个元组 $(\pmb t,\pmb c)$，返回 $\pmb c$ 是 $\pmb t$ 的上下文词的概率值
$$
P(+|\pmb t,\pmb c)
$$
如何计算这个概率值？skip-gram模型假定与相似度有关：<u>如果一个词的嵌入与目标词的嵌入相似，则该词可能出现在目标词附近</u>。我们还是用内积衡量两个向量的相似度
$$
{\rm similarity}(\pmb t,\pmb c)=\pmb t\cdot\pmb c
$$

> 注意“相似"概念：tf-idf模型的相似指二阶共现（即语义相似），而skip-gram模型的相似指一阶共现（即彼此相邻），衡量相似的方法都是计算内积（余弦相似度可以视为归一化的内积）。换言之，tf-idf模型的向量空间中同义词彼此靠近，而skip-gram模型的向量空间中相邻出现的词彼此靠近。

内积取值在正负无穷之间，因此采用逻辑函数映射到(0,1)区间
$$
P(+|\pmb t,\pmb c)=\frac{1}{1+e^{-\pmb t\cdot \pmb c}}\\
P(-|\pmb t,\pmb c)=\frac{1}{1+e^{\pmb t\cdot \pmb c}}
$$
当有多个候选词时，skip-gram模型假设上下文的所有词都是独立的，因此概率可以相乘：
$$
P(+|\pmb t,\pmb c_{1:k})=\prod_{i=1}^k\frac{1}{1+e^{-\pmb t\cdot \pmb c_i}}\\
\log P(+|\pmb t,\pmb c_{1:k})=-\sum_{i=1}^k\log(1+e^{-\pmb t\cdot \pmb c_i})
$$
总之，skip-gram模型训练了这样一个分类器，给定目标词 $\pmb t$ 和包含 $k$ 个词 $\pmb c_{1:k}$ 的窗口，它可以返回反映目标词和窗口的相似程度的概率值。概率值基于目标词和每一个上下文词的嵌入的内积，因此我们需要对词汇表的所有词进行嵌入。



## skip-gram嵌入

word2vec学习嵌入的方法是，首先随机初始化一组嵌入向量，然后迭代地改变每个词的嵌入，使其更接近于词的上下文词的嵌入，而远离非上下文词的嵌入。还是考虑这个句子：

![](https://i.loli.net/2021/01/07/QDA5nCiR4Sbx3y7.png)

从中可以得到4个正例，如下图所示：

![](https://i.loli.net/2021/01/07/oAKJN2GHbPcyCOD.png)

为了训练分类器我们还需要反例，实际上我们使用 $k$ 倍于正例数量的反例（ $k$ 是超参数，这里取2）。反例中的每个候选词（也称为噪声词）从词汇表随机选择得到，服从加权的unigram频率分布：
$$
p_\alpha(w)=\frac{{\rm count}(w)^\alpha}{\sum_{w'}{\rm count}(w')^\alpha}
$$
实践中 $\alpha$ 通常取0.75，可以让各概率值向中间靠拢，提高罕见词的概率。例如假设长度为100的文本中出现了99次a，1次b，那么
$$
p_\alpha(a)=\frac{.99^.75}{.99^.75+.01^.75}=.97\\
p_\alpha(b)=\frac{.01^.75}{.99^.75+.01^.75}=.03
$$
现在我们有了初始的嵌入和正反例，接下来就是学习算法对嵌入进行调整，目标为：

+ 最大化正例的元组的相似度
+ 最小化反例的元组的相似度

因此优化问题的目标函数为
$$
\max\mathcal{L}(\pmb \theta)=\sum_{(\pmb t,\pmb c)\in +}\log P(+|\pmb t,\pmb c)+\sum_{(\pmb t,\pmb c)\in -}\log P(-|\pmb t,\pmb c)\\
=-\sum_{(\pmb t,\pmb c)\in +}\log(1+e^{-\pmb t\cdot \pmb c})-\sum_{(\pmb t,\pmb c)\in -}\log(1+e^{\pmb t\cdot \pmb c})\\
即\min \mathcal{L}=\sum_{(\pmb t,\pmb c)\in +}\log(1+e^{-\pmb t\cdot \pmb c})+\sum_{(\pmb t,\pmb c)\in -}\log(1+e^{\pmb t\cdot \pmb c})\\
$$
可以使用随机梯度下降法进行优化。

注意skip-gram模型为每个词分别学习的两个嵌入：**目标嵌入(target embedding)**和**上下文嵌入(context embedding)**，分别用于词作为目标词和词作为其它目标词的上下文词。所有嵌入存储在两个矩阵：目标矩阵 $T$ 和上下文矩阵 $C$ 中，如下图所示。矩阵 $T,C$ 即为模型参数，其中 $d$ 为超参数。

![](https://i.loli.net/2021/01/07/nEphgvjNyeGuVYt.png)

训练完毕之后，我们可以用 $\pmb t_i$ 作为词 $w_i$ 的嵌入，也可以用 $\pmb t_i+\pmb c_i$，还可以选择 $\pmb t_i\oplus\pmb c_i$ （ $\oplus$ 表示向量拼接）。

窗口长度 $L$ 也是一个重要的超参数，通常根据验证集调参。 $L$ 越大，则训练集（正反例）的规模越大。





# 嵌入可视化

嵌入的可视化对于我们理解、应用和改进嵌入方法非常重要，但我们应该如何可视化一个高维向量呢？

可视化一个词的最简单方法是列出与该词最相似的几个词（通过计算余弦相似度），例如GloVe嵌入中最接近frog的7个词分别为frogs, toad, litoria, leptodactylidae, rana, lizard, 和eleutherodactylus (Pennington et al., 2014)。

另一个方法是为嵌入空间中的所有词构造一个最小生成树，如下图所示：

![](https://i.loli.net/2021/01/07/atwWBexA6zofCq9.png)

但最常用的可视化方法依然是投影，例如课本上将高维空间投影到二维，tensorboard可以将高维空间投影到三维。如图6.13使用了一种名为t-SNE的投影方法(van der Maaten and Hinton, 2008)。





# 嵌入的语义性质

向量语义模型的一个很重要的超参数就是窗口长度，tf-idf和word2vec模型都有这个参数。对于这个参数的选择取决于表示的目标：短窗口得到的相似的词与目标词在语义上相似，而长窗口得到的高余弦相似度的词则只与目标词在话题上相关而并不一定相似。

例如Levy and Goldberg (2014a)展示了，使用skip-gram模型和±2窗口时，与Hogwarts（出自哈利波特系列）最相似的词是其它小说的学校：Sunnydale（出自吸血鬼猎人巴菲）和Evernight（出自某个吸血鬼系列）；使用±5窗口时，余弦相似度最高的词则是哈利波特相关话题下的词：Dumbledore, Malfoy和half-blood。

之前的相似特指语义相似，为便于表达，这里将相似再细分为两种(Schütze and Pedersen, 1993)：**一阶共现(first-order co-occurrence)**，也称为**组合关联(syntagmatic association)**指两个词通常彼此相邻，例如write与book或poem一阶共现；**二阶共现(second-order co-occurrence)**，也称为**聚合关联(paradigmatic association)**指两个词有相似的上下文词，例如write与say或remark二阶共现。

嵌入的另一个语义性质就是可以抓住词义关系。Mikolov et al. (2013b)和Levy and Goldberg (2014b)展示了嵌入向量的偏移捕获了一些词之间的**类比(analogy)**关系，例如，vector(king)-vector(man)+vector(woman)的结果非常接近vector(queen)，下图将这一关系在二维投影空间中进行了可视化：

![](https://i.loli.net/2021/01/07/Kn2Qe3tXzCB97P6.png)

嵌入对于研究语义随时间的变化也十分有用，通过计算不同年代的文本分别对应的嵌入空间。例如下图展示了英文词在两个世纪内的词义变化：

![](https://i.loli.net/2021/01/07/ID98XkhcBioC54N.png)





# 偏差

嵌入会复现文本中潜在的偏见和刻板印象。例如Bolukbasi et al. (2016)发现computer programmer - man + woman的结果最接近于homemaker；以前的**内隐关联测试(Implicit Association Test, IAT)**发现，美国人会将非裔美国人的名字关联到不愉快的词（相对于欧洲裔美国人），将男性关联到数学而女性关联到艺术，将老人的名字关联到不愉快的词(Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b)，而Caliskan et al. (2017)通过GloVe嵌入和计算余弦相似度也复现了上述发现。

最近的研究致力于移除这些偏见……但这些**纠偏(debias)**方法只能降低偏见，而不能将其完全去除(Gonen and Goldberg, 2019)。

历史文本的嵌入也会包含过去的偏见和刻板印象。





# 评价向量模型

最重要的评价指标自然是外部评价：将它们作为NLP任务的特征并观察是否相对于其它向量模型有更好的表现。

尽管如此也有一些内部评价方法，最常用的指标是测试向量模型的相似度表现，即对于一些给定的词对，计算模型打分和人工打分的相关系数。**WordSim-353**(Finkelstein et al., 2002)就是一个常用的集合，包含了353个打分从0到10的名词对；**SimLex-999**(Hill et al., 2015)则是一个更复杂的数据集，包括具体和抽象的形容词、名词和动词对；**TOEFL数据集**包含80道选择同义词的题目，例如Levied的含义最接近于: imposed, believed, requested, correlated (Landauer and Dumais, 1997)。这些数据集给出的词均没有上下文。

更实际一点的内部相似度任务包含上下文。**Stanford Contextual Word Similarity(SCWS)**数据集(Huang et al., 2012)给出了人对于2003对词在句子上下文中的评价；**Word-in-Context(WiC)**数据集(Pilehvar and Camacho-Collados, 2019)给出了目标词和两个包含该词的句子，需要判断词在这两个上下文中的词义是相同的还是不同的。语义文本相似度任务(Agirre et al. 2012, Agirre et al. 2015)则评价句子级别的相似度算法，使用人工打分相似度的句子对的集合。

还有一种任务称为类比任务，例如对于“雅典之于希腊相当于奥斯陆之于___”，系统应该返回“挪威“。已经有大型的这种元组数据集被创建(Mikolov et al. 2013, Mikolov et al. 2013b)。

