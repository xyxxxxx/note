# 熵、相对熵与互信息

## 熵

熵是随机变量不确定性的度量。设 $X$ 是一个<u>离散型</u>随机变量，其取值空间为 $\mathcal{X}$，概率密度函数（概率分布）$p(x)={\rm Pr}(X=x),x\in\mathcal{X}$。

一个离散型随机变量 $X$ 的**熵（entropy）**$H(X)$ 定义为

$$
H(X)=-\sum_{x\in\mathcal{X}}p(x)\log_2 p(x)
$$

熵的单位用比特（位）表示。例如，抛掷均匀硬币这一事件的熵为 1 比特。由于当 $x\to 0$ 时，$x\log x\to 0$，我们约定 $0\log 0=0$，这意味着加上零概率的项不改变熵的值。

!!! note "说明"
    熵实际上是随机变量 $X$ 的分布的泛函（和期望、方差一样），并不依赖于 $X$ 的实际取值，而仅依赖于其概率分布。

$X$ 的熵又解释为随机变量 $\log \frac{1}{p(X)}$ 的期望，于是

$$
H(X)=E\log\frac{1}{p(x)}
$$

引理 1：

$$
H(X)\ge 0
$$

证明：

由 $0\le p(x)\le 1$ 有 $-\log{p(x)}\ge 0$，有 $-p(x)\log{p(x)}\ge 0$

@设 $P(X=1)=p$，$P(X=0)=1-p$，则

$$
H(X)=-p\log p-(1-p)\log (1-p)\stackrel{\text{def}}{=}H(p)
$$

函数 $H(p)$ 的图形如下，它说明了熵的一些基本性质：

* $H(p)$ 为上凸函数
* 当 $p=0$ 或 $1$ 时，$H(p)=0$，表示变量不再是随机的，因而不再具有不确定性
* 当 $p=\frac{1}{2}$ 时，变量的不确定性达到最大，此时对应于熵也取最大值

![](https://s2.loli.net/2022/12/22/KTx4QYPwHmdan8G.png)

@假定有 8 匹马参加一场赛马比赛，这 8 匹马的获胜概率分布为 $(\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{64},\frac{1}{64},\frac{1}{64},\frac{1}{64})$，则这场赛马的熵为

$$
H(X)=-\frac{1}{2}\log\frac{1}{2}-\frac{1}{4}\log\frac{1}{4}-\frac{1}{8}\log\frac{1}{8}-\frac{1}{16}\log\frac{1}{16}-4\frac{1}{64}\log\frac{1}{64}=2比特
$$

假定我们要把哪匹马会获胜的消息发送出去，其中一个策略是发送胜出马的编号，这样对任意一匹马，描述需要固定 3 比特。但由于获胜的概率不是均等的，因此明智的方法是对获胜概率较大的马使用较短的描述，而对获胜概率较小的马使用较长的描述。这样做，我们会获得一个更短的平均描述长度。例如，使用以下的一组二元字符串来表示 8 匹马：0、10、110、1110、111100、111101、111110、111111。此时平均描述长度为 2 比特（正好等于熵），小于等长编码的 3 比特。

## 联合熵和条件熵

现在我们将定义推广到两个随机变量的情形。由于可将 $(X,Y)$ 视为一个随机向量，所以定义其实并无新鲜之处。

对于服从联合分布为 $p(x,y)$ 的一对离散型随机变量 $(X,Y)$，其**联合熵（joint entropy）**$H(X,Y)$ 定义为

$$
H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(x,y)
$$

上式亦可表示为

$$
H(X,Y)=E\log \frac{1}{p(x,y)}
$$

也可以定义一个随机变量在给定另一个随机变量下的条件熵，它是条件分布熵关于起条件作用的那个随机变量取平均之后的期望值。

对于服从联合分布为 $p(x,y)$ 的一对离散型随机变量 $(X,Y)$，其**条件熵（joint entropy）**$H(Y|X)$ 定义为

$$
\begin{align}
H(Y|X)&=\sum_{x\in\mathcal{X}}p(x)H(Y|X=x)\\
&=-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y|x)\log p(y|x)\\
&=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(y|x)\\
&=-E\log p(Y|X)
\end{align}
$$


